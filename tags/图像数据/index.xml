<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>图像数据 on RuHeYun</title>
        <link>https://ruheyun.github.io/tags/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE/</link>
        <description>Recent content in 图像数据 on RuHeYun</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>RuHeYun</copyright>
        <lastBuildDate>Thu, 29 Jan 2026 17:05:17 +0800</lastBuildDate><atom:link href="https://ruheyun.github.io/tags/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Diffusion</title>
        <link>https://ruheyun.github.io/p/diffusion/</link>
        <pubDate>Thu, 29 Jan 2026 15:55:20 +0800</pubDate>
        
        <guid>https://ruheyun.github.io/p/diffusion/</guid>
        <description>&lt;h1 id=&#34;扩散模型笔记1&#34;&gt;扩散模型笔记(1)
&lt;/h1&gt;&lt;h2 id=&#34;论文基本信息&#34;&gt;论文基本信息
&lt;/h2&gt;&lt;p&gt;&lt;mark&gt;论文名称&lt;/mark&gt;：Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;出版期刊&lt;/mark&gt;：ICML 2015&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;学校机构&lt;/mark&gt;：Stanford University，University of California, Berkeley&lt;/p&gt;
&lt;h2 id=&#34;问题与动机&#34;&gt;问题与动机
&lt;/h2&gt;&lt;p&gt;机器学习中的概率模型一直面临 “灵活性” vs “可处理性” (Flexibility vs. Tractability) 的权衡。&lt;/p&gt;
&lt;p&gt;简单模型 (如高斯分布): 易于计算（采样、求概率、推断），但无法捕捉复杂数据（如图像）的丰富结构。&lt;/p&gt;
&lt;p&gt;灵活模型 (如能量模型 $p(x) \varpropto \phi(x)$): 理论上可以拟合任意分布，但归一化常数 Z 难以计算，导致训练、采样、评估都极其困难（通常依赖昂贵的 MCMC)。&lt;/p&gt;
&lt;h2 id=&#34;该论文方法&#34;&gt;该论文方法
&lt;/h2&gt;&lt;p&gt;提出一种新框架，同时实现高度灵活性和计算可处理性。&lt;/p&gt;
&lt;p&gt;快速学习复杂的深度生成模型。&lt;/p&gt;
&lt;p&gt;精确采样 (Exact Sampling)。&lt;/p&gt;
&lt;p&gt;廉价地评估数据点的概率 (Cheap Probability Evaluation)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;轻松计算后验概率（用于去噪、修复等任务）。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark style=&#34;background-color: #00ffff;&#34;&gt;关键思想：与其直接建模复杂的$p(x)$，不如建模一个过程&amp;ndash;一个能将简单噪声变回数据的过程。&lt;/mark&gt;&lt;/p&gt;
&lt;h2 id=&#34;核心思想&#34;&gt;核心思想
&lt;/h2&gt;&lt;p&gt;受物理启发的扩散过程&lt;/p&gt;
&lt;p&gt;&lt;mark style=&#34;background-color: #00ffff;&#34;&gt;前向过程&lt;/mark&gt; (Forward/Inference Process - q): 这是一个预定义的、固定的&lt;strong&gt;马尔可夫链&lt;/strong&gt;。它从真实数据 $x(0) \sim q(x(0))$ 开始。通过 $T$ 步逐渐添加噪声（例如高斯噪声），最终将任何复杂的数据分布平滑地转化为一个简单的、易于处理的先验分布（例如标准高斯分布 $\pi(x(T)) = N(0,I)$)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目的： “破坏”数据中的结构，使其变得无序（像热力学中的熵增过程）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键点&lt;/strong&gt;： 这个过程是已知的、可解析计算的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;mark style=&#34;background-color: #00ffff;&#34;&gt;反向过程&lt;/mark&gt; (Reverse/Generative Process - p): 要学习的模型。它从简单的先验分布 $x(T) \sim p(x(T)) = \pi(x(T))$ 开始。通过 $T$ 步逐渐去除噪声，最终重构出数据分布 $p(x(0))$ 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目的： “恢复”数据中的结构，进行生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键点&lt;/strong&gt;： 每一步的转换 $p(x(t-1) \mid x(t))$ 由一个神经网络（如MLP或CNN）参数化，用于预测均值（和方差）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;mark style=&#34;background-color: #00ffff;&#34;&gt;非平衡态热力学 &lt;/mark&gt;(Nonequilibrium Thermodynamics): 前向过程就像一个非平衡的“淬火”过程，系统从有序（数据）变为无序（噪声）。&lt;/p&gt;
&lt;p&gt;&lt;mark style=&#34;background-color: #00ffff;&#34;&gt;准静态过程&lt;/mark&gt; (Quasi-static Process): 如果 T 足够大（即每一步的噪声添加/移除非常微小），那么前向和反向过程的轨迹会变得几乎相同。这使得模型的训练和评估变得非常高效。&lt;/p&gt;
&lt;h2 id=&#34;优势&#34;&gt;优势
&lt;/h2&gt;&lt;p&gt;灵活性：通过深度网络和长链条，可以建模极其复杂的分布。&lt;/p&gt;
&lt;p&gt;可处理性：采样（从后往前跑一遍）、概率评估（用重要性采样）、训练（优化下界）都是高效的。&lt;/p&gt;
&lt;p&gt;&lt;mark style=&#34;background-color: #00ffff;&#34;&gt;后验推断&lt;/mark&gt;：非常重要！由于模型是基于过程的，要计算后验 $p(x(0) \mid y) \propto p(x(0)) * r(x(0))$ （例如 r 是一个观测模型），只需要对反向过程的每一步进行微小的扰动即可。&lt;strong&gt;可以推出是对均值的扰动&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;困惑点&#34;&gt;困惑点
&lt;/h2&gt;&lt;p style=&#34;color: red;&#34;&gt;为什么论文第二部分又称前向过程为 Inference Process？&lt;/p&gt;
&lt;p&gt;在这篇论文中，“inference” 指的是：给定一个真实数据样本 x(0) ，推断（或构造）它在整个扩散轨迹中对应的噪声版本序列 x(1),x(2),…,x(T) 。这里的 “inference” 不是指“从训练好的模型生成新样本”，而是指“为训练目的，从已知数据反推其在扩散链中的演化路径”。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
