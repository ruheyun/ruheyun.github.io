<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Membership Inference Attacks on RuHeYun</title>
        <link>https://ruheyun.github.io/tags/membership-inference-attacks/</link>
        <description>Recent content in Membership Inference Attacks on RuHeYun</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>RuHeYun</copyright>
        <lastBuildDate>Fri, 13 Feb 2026 17:27:10 +0800</lastBuildDate><atom:link href="https://ruheyun.github.io/tags/membership-inference-attacks/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>针对扩散模型的白盒成员推理攻击</title>
        <link>https://ruheyun.github.io/p/gsa/</link>
        <pubDate>Fri, 13 Feb 2026 16:18:06 +0800</pubDate>
        
        <guid>https://ruheyun.github.io/p/gsa/</guid>
        <description>&lt;p&gt;扩散模型阅读笔记&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h1 id=&#34;论文基本信息&#34;&gt;论文基本信息
&lt;/h1&gt;&lt;p&gt;&lt;mark&gt;论文名称&lt;/mark&gt;：White-box Membership Inference Attacks against Diffusion Models&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;出版期刊&lt;/mark&gt;：PoPETs 2025&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;学校机构&lt;/mark&gt;：University of Virginia, Iowa State University, CISPA Helmholtz Center for  Information Security&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;White-box（白盒）：指攻击者能够完全访问模型的内部参数、结构和训练过程，与“黑盒”（仅能通过输入输出接口交互）相对。&lt;/p&gt;
&lt;p&gt;Membership Inference Attacks（成员推理攻击）：一种隐私攻击方法，旨在判断某个特定数据样本是否曾被用于训练目标模型，从而泄露训练数据的隐私信息。&lt;/p&gt;
&lt;p&gt;Diffusion Models（扩散模型）：一类基于逐步加噪和去噪过程的生成式模型，广泛应用于图像、音频等数据生成任务。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;论文翻译&#34;&gt;论文翻译
&lt;/h1&gt;&lt;h2 id=&#34;摘要&#34;&gt;摘要
&lt;/h2&gt;&lt;p&gt;扩散模型凭借卓越的图像生成能力，已在工业应用中逐渐超越GAN等其他生成式模型。此类模型的复杂架构蕴含丰富的攻击特性。鉴于此，&lt;u&gt;本研究旨在设计面向扩散模型的成员推理攻击（MIAs）&lt;/u&gt;。我们首先对现有攻击方法进行系统性分析，综合考量黑盒/白盒模型类型与攻击特征选择等因素。实验发现：&lt;u&gt;白盒攻击在真实场景中具有高度适用性，且当前最有效的攻击形式即为白盒攻击&lt;/u&gt;。与早期研究不同——后者以模型损失作为白盒攻击的核心特征，&lt;mark&gt;本研究创新性地采用模型梯度作为攻击特征，充分利用梯度信息能更深入反映模型对各类样本响应机制的特性&lt;/mark&gt;。我们通过严格的参数测试（涵盖训练步数、时间步采样频率、扩散步数及数据方差等），验证了该方法的鲁棒性。在所有实验设置下，攻击成功率均趋近100%，攻击AUCROC值接近1.0。此外，针对常见防御机制的测试表明，本攻击仍能保持显著效果。代码 &lt;a class=&#34;link&#34; href=&#34;https://github.com/py85252876/GSA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;p&gt;近年来，扩散模型受到广泛关注，各类应用不断涌现。这类模型依赖于渐进式去噪过程生成图像，相较于早期的生成对抗网络（GANs）和变分自编码器（VAEs），其生成图像的质量显著提升。当前主流模型主要分为两类：第一类为基于扩散机制的架构，包括 GLIDE、Stable Diffusion、DALL·E 2和 Imagen；第二类为代表性的序列到序列（sequence-to-sequence）模型，如 DALL·E、Parti 和 CogView。现有的文本到图像生成模型能够根据文本描述生成精致且细节丰富的图像，已广泛应用于平面设计、插画创作等多个领域。尽管扩散模型可用于合成独特的艺术风格，但其训练通常需要依赖大规模敏感数据集。因此，研究成员推理攻击（Membership Inference Attacks, MIAs）—— 即判断特定样本是否曾被用于扩散模型训练——对于保障数据隐私具有至关重要的意义。&lt;/p&gt;
&lt;p&gt;针对分类模型与生成对抗网络（GANs）等模型的成员推理攻击已开展了大量研究。然而，由于扩散模型在训练与推理机制上具有独特性，既有的攻击方法难以直接适用。例如，在分类模型中，攻击通常以模型的最终输出作为特征，依赖模型对训练数据的过拟合所导致的分类置信度差异进行推断；而针对 GAN 等生成模型的早期研究则主要利用其判别器（discriminator）进行成员判定。由于扩散模型本身不包含判别器，其架构与 GAN 存在本质差异，因此必须为其专门设计新型攻击方法。&lt;/p&gt;
&lt;p&gt;目前已有一些初步工作尝试对扩散模型实施成员推理攻击。但值得注意的是，这些研究与该领域多数工作类似，主要聚焦于基于损失函数（loss-based）和阈值判定（threshold-based）的攻击策略。我们认为，神经网络的不同层学习到的特征各异，因而存储的信息量也存在差异。仅依赖损失值进行评估可能忽略大量潜在的判别性信息。因此，若在考虑模型损失的同时，进一步纳入反向传播后各网络层的梯度信息，将有助于更全面地刻画模型对样本的响应特性，从而提升攻击的有效性。&lt;/p&gt;
&lt;p&gt;将梯度信息用于成员推理攻击（MIAs）面临两大主要挑战：一是计算开销过大，二是攻击模型易出现过拟合问题（鉴于扩散模型规模庞大，其梯度维度可达数百万）。为此，我们深入分析了降维策略，提出了一种融合子采样（subsampling）与聚合（aggregation）机制的攻击框架，命名为&lt;strong&gt;基于子采样与聚合的梯度攻击&lt;/strong&gt;（Gradient attack based on Subsampling and Aggregation, &lt;strong&gt;GSA&lt;/strong&gt;），并进一步设计了两种具体实现方案——GSA1 与 GSA2，以展示该框架内不同设计权衡下的性能表现。&lt;/p&gt;
&lt;p&gt;为确保研究的全面性与完整性，我们在两类代表性扩散模型上开展实验：基础的无条件去噪扩散概率模型（Denoising Diffusion Probabilistic Models, &lt;strong&gt;DDPM&lt;/strong&gt;）与当前文本到图像生成领域的前沿模型 &lt;strong&gt;Imagen&lt;/strong&gt;。其中，CIFAR-10 与 ImageNet 数据集用于训练无条件扩散模型，MS COCO 数据集用于训练 Imagen 模型。我们系统探究了不同参数（如时间步、模型层数等）对攻击效果的影响。最终实验结果表明，所提攻击策略取得了接近 100% 的成功率，充分揭示了扩散模型在安全性方面亟待解决的隐私风险。&lt;/p&gt;
&lt;p&gt;本文的主要贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;系统梳理了现有针对扩散模型的成员推理攻击研究，并面向新型实用场景设计了基于梯度的攻击方法，从时间步、模型层等多个维度展开深入分析；&lt;/li&gt;
&lt;li&gt;在三种数据集上分别基于传统 DDPM 模型与前沿文本到图像模型 Imagen 进行实验验证。结果表明，该方法在四项评估指标上均达到极高准确率，有力证实了将梯度作为攻击特征的有效性与优越性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;论文结构安排&lt;/strong&gt;。第 2 节介绍扩散模型的背景知识，并深入探讨成员推理攻击的原理；同时分析我们面临的主要挑战，并回顾现有针对扩散模型的攻击方法。第 3 节详细阐述我们提出的攻击策略。第 4 节说明实验设置，第 5 节展示实验结果。第 6 节将 GSA 框架应用于模型层级别，进一步降低计算耗时。第 7 节验证该攻击在多种防御策略下的性能表现。第 8 节讨论本攻击方法的局限性。第 9 节综述相关研究工作，最后第 10 节对全文进行总结。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
