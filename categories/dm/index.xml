<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DM on RuHeYun</title>
        <link>https://ruheyun.github.io/categories/dm/</link>
        <description>Recent content in DM on RuHeYun</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>RuHeYun</copyright><atom:link href="https://ruheyun.github.io/categories/dm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>论文阅读汇总</title>
        <link>https://ruheyun.github.io/p/paper-reading/</link>
        <pubDate>Fri, 06 Feb 2026 14:55:22 +0800</pubDate>
        
        <guid>https://ruheyun.github.io/p/paper-reading/</guid>
        <description>&lt;img src="https://ruheyun.github.io/p/paper-reading/123.jpg" alt="Featured image of post 论文阅读汇总" /&gt;&lt;h2 id=&#34;论文翻译笔记&#34;&gt;论文翻译笔记
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ruheyun.github.io/p/diffusion&#34; &gt;Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/a&gt; 2015【已更完】&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ruheyun.github.io/p/ncsn&#34; &gt;Generative Modeling by Estimating Gradients of the Data Distribution&lt;/a&gt; 2019【已更完】&lt;/p&gt;
&lt;h2 id=&#34;论文阅读笔记&#34;&gt;论文阅读笔记
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ruheyun.github.io/p/gsa&#34; &gt;White-box Membership Inference Attacks against Diffusion Models&lt;/a&gt; 2025【更新中】&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GSA</title>
        <link>https://ruheyun.github.io/p/gsa/</link>
        <pubDate>Fri, 13 Feb 2026 16:18:06 +0800</pubDate>
        
        <guid>https://ruheyun.github.io/p/gsa/</guid>
        <description>&lt;p&gt;扩散模型阅读笔记&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;论文基本信息&#34;&gt;论文基本信息
&lt;/h2&gt;&lt;p&gt;&lt;mark&gt;论文标题&lt;/mark&gt;：White-box Membership Inference Attacks against Diffusion Models&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;出版期刊&lt;/mark&gt;：PoPETs 2025&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;论文作者&lt;/mark&gt;：Yan Pang, Tianhao Wang, Xuhui Kang, Mengdi Huai, Yang Zhang&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;学校机构&lt;/mark&gt;：University of Virginia, Iowa State University, CISPA Helmholtz Center for  Information Security&lt;/p&gt;
&lt;h2 id=&#34;目标贡献&#34;&gt;目标/贡献
&lt;/h2&gt;&lt;p&gt;设计针对&lt;code&gt;扩散模型&lt;/code&gt;的&lt;code&gt;白盒成员&lt;/code&gt;推理攻击（MIA），利用&lt;code&gt;梯度信息&lt;/code&gt;提升攻击效果；&lt;/p&gt;
&lt;p&gt;提出 GSA 框架及实例，验证梯度作为攻击特征的优越性，多数据集/模型验证有效性&lt;/p&gt;
&lt;h2 id=&#34;研究背景&#34;&gt;研究背景
&lt;/h2&gt;&lt;h3 id=&#34;扩散模型现状&#34;&gt;扩散模型现状
&lt;/h3&gt;&lt;p&gt;优势：图像生成性能超 GANs/VAEs，应用于图形设计等领域&lt;/p&gt;
&lt;p&gt;分类：无条件（DDPM）、条件（Imagen、Stable Diffusion）&lt;/p&gt;
&lt;p&gt;隐患：依赖敏感训练数据，存在隐私泄露风险&lt;/p&gt;
&lt;h3 id=&#34;成员推理攻击mia基础&#34;&gt;成员推理攻击（MIA）基础
&lt;/h3&gt;&lt;p&gt;定义：一种隐私攻击方法，旨在判断某个特定数据样本是否曾被用于训练目标模型，从而泄露训练数据的隐私信息。&lt;/p&gt;
&lt;p&gt;现有方法局限：分类模型依赖输出向量，GANs依赖判别器，扩散模型无判别器需新方法&lt;/p&gt;
&lt;h3 id=&#34;黑盒白盒攻击&#34;&gt;黑盒/白盒攻击
&lt;/h3&gt;&lt;p&gt;White-box（白盒）：指攻击者能够完全访问模型的内部参数、结构和训练过程，与“黑盒”（仅能通过输入输出接口交互）相对。&lt;/p&gt;
&lt;p&gt;我们发现白盒攻击在现实世界中非常适用，目前最有效的攻击是白盒。&lt;/p&gt;
&lt;h3 id=&#34;现有扩散模型-mia-不足&#34;&gt;现有扩散模型 MIA 不足
&lt;/h3&gt;&lt;p&gt;主流依赖损失值/阈值，信息单一易误判&lt;/p&gt;
&lt;p&gt;白盒攻击为当前最有效类型，但现有方法计算成本高&lt;/p&gt;
&lt;h2 id=&#34;攻击方法设计gsa-框架&#34;&gt;攻击方法设计（GSA 框架）
&lt;/h2&gt;&lt;h3 id=&#34;思想&#34;&gt;思想
&lt;/h3&gt;&lt;p&gt;用梯度替代损失作为攻击特征，梯度含更高维模型响应信息&lt;/p&gt;
&lt;h3 id=&#34;理论基础&#34;&gt;理论基础
&lt;/h3&gt;&lt;p&gt;梯度公式：$∇_θL_t(θ,x) = 2(ε_θ(x_t,t)-εt)⊤∇_θε_θ(x_t,t)$&lt;/p&gt;
&lt;p&gt;优势：即使损失相同，梯度可通过 $∇θεθ(xt,t)$ 区分成员/非成员样本&lt;/p&gt;
&lt;h3 id=&#34;梯度降维策略&#34;&gt;梯度降维策略
&lt;/h3&gt;&lt;p&gt;时间步层面：等距采样（平衡效果与效率）、有效采样（需预计算黄金区间）、泊松采样（随机）&lt;/p&gt;
&lt;p&gt;模型层层面：选择性提取各层梯度，聚合有用信息&lt;/p&gt;
&lt;p&gt;层内梯度层面：将梯度视为集合，避免顺序依赖&lt;/p&gt;
&lt;h3 id=&#34;具体实例&#34;&gt;具体实例
&lt;/h3&gt;&lt;p&gt;GSA₁：先求多时间步损失均值，再反向传播求梯度（效率高，Imagen耗时&amp;lt;2小时）&lt;/p&gt;
&lt;p&gt;GSA₂：逐时间步求梯度，再取均值（效果优，耗时约6小时）&lt;/p&gt;
&lt;h2 id=&#34;实验&#34;&gt;实验
&lt;/h2&gt;&lt;p&gt;代码 &lt;a class=&#34;link&#34; href=&#34;https://github.com/py85252876/GSA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github&lt;/a&gt;。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>NCSN</title>
        <link>https://ruheyun.github.io/p/ncsn/</link>
        <pubDate>Wed, 04 Feb 2026 14:56:05 +0800</pubDate>
        
        <guid>https://ruheyun.github.io/p/ncsn/</guid>
        <description>&lt;h1 id=&#34;扩散模型阅读笔记2&#34;&gt;扩散模型阅读笔记(2)
&lt;/h1&gt;&lt;h2 id=&#34;论文基本信息&#34;&gt;论文基本信息
&lt;/h2&gt;&lt;p&gt;&lt;mark&gt;论文名称&lt;/mark&gt;：Generative Modeling by Estimating Gradients of the Data Distribution&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;出版期刊&lt;/mark&gt;：NeurIPS 2019&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;学校机构&lt;/mark&gt;：Stanford University&lt;/p&gt;
&lt;h2 id=&#34;论文翻译&#34;&gt;论文翻译
&lt;/h2&gt;&lt;h3 id=&#34;标题&#34;&gt;标题
&lt;/h3&gt;&lt;p&gt;基于数据分布梯度估计的生成模型&lt;/p&gt;
&lt;h3 id=&#34;摘要&#34;&gt;摘要
&lt;/h3&gt;&lt;p&gt;我们提出了一种新型生成模型，&lt;mark&gt;该模型通过朗之万动力学（Langevin dynamics）进行采样，所用梯度通过对数据分布进行分数匹配（score matching）来估计&lt;/mark&gt;。&lt;u&gt;由于当数据位于低维流形上时，梯度可能难以定义且难以准确估计，我们采用不同强度的高斯噪声对数据进行扰动，并联合估计对应各噪声层级的分数（score），即所有噪声水平下扰动后数据分布的梯度向量场&lt;/u&gt;。&lt;mark&gt;在采样阶段，我们提出了一种退火朗之万动力学（annealed Langevin dynamics）：随着采样过程逐渐逼近真实数据流形，我们逐步使用对应更低噪声水平的梯度进行迭代更新&lt;/mark&gt;。代码实现&lt;a class=&#34;link&#34; href=&#34;https://github.com/ermongroup/ncsn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;引言&#34;&gt;引言
&lt;/h3&gt;&lt;p&gt;生成模型在机器学习中具有广泛的应用。举例而言，它们已被用于生成高保真图像、合成逼真的语音与音乐片段、提升半监督学习的性能、检测对抗样本及其他异常数据、模仿学习，以及在强化学习中探索具有潜力的状态空间。近年来的进展主要由两类方法推动：基于似然的方法（likelihood-based methods）与生成对抗网络（GAN）。前者以对数似然（或合适的代理目标）作为训练目标，而后者则通过对抗训练来最小化模型分布与数据分布之间的 f 散度（f-divergences）或积分概率度量（integral probability metrics）。&lt;/p&gt;
&lt;p&gt;尽管基于似然的模型与生成对抗网络（GAN）已取得显著成功，但它们各自存在一些固有局限。例如，基于似然的模型要么必须采用特定架构以构建归一化的概率模型（如自回归模型、流模型），要么需借助代理损失函数进行训练（如变分自编码器中使用的证据下界 、基于能量模型中的对比散度）。GAN 在一定程度上规避了基于似然模型的部分局限，但其对抗训练过程往往导致训练不稳定。此外，GAN 的目标函数并不适合用于不同 GAN 模型之间的评估与比较。尽管生成建模领域还存在其他目标函数，例如噪声对比估计（noise contrastive estimation）与最小概率流（minimum probability flow），但这些方法通常仅适用于低维数据，在高维场景下表现欠佳。&lt;/p&gt;
&lt;p&gt;本文探索了一种基于估计与采样数据对数密度（logarithmic data density）的（Stein）分数的新型生成建模原理。&lt;mark&gt;该分数定义为输入数据点处对数密度函数的梯度，构成一个指向对数数据密度增长最快方向的向量场&lt;/mark&gt;。我们采用基于&lt;u&gt;分数匹配（score matching）&lt;/u&gt;训练的神经网络，从数据中学习这一向量场，并进一步利用&lt;u&gt;朗之万动力学（Langevin dynamics）&lt;/u&gt;生成样本：其基本原理是将随机初始样本沿着（估计得到的）分数向量场逐步移向高密度区域。然而，该方法面临&lt;span style=&#34;color: red;&#34;&gt;两大主要挑战&lt;/span&gt;：&lt;code&gt;首先&lt;/code&gt;，若数据分布支撑于低维流形上——这在许多真实世界数据集中是常见假设——则在环境空间（ambient space）中分数将无法良好定义，导致分数匹配无法提供一致的分数估计器。&lt;code&gt;其次&lt;/code&gt;，在低数据密度区域（例如远离流形的区域）训练数据稀缺，这不仅制约了分数估计的准确性，也延缓了朗之万动力学采样的混合（mixing）过程。由于朗之万动力学通常在数据分布的低密度区域进行初始化，这些区域中不准确的分数估计将对采样过程产生负面影响。此外，为实现在分布不同模态（modes）之间的转移，采样过程往往需要穿越低密度区域，这也使得混合过程变得困难。&lt;/p&gt;
&lt;p&gt;为应对上述两大挑战，&lt;mark&gt;我们提出对数据施加不同强度的随机高斯噪声进行扰动。添加随机噪声可确保所得分布不会坍缩至低维流形&lt;/mark&gt;。&lt;u&gt;较大的噪声强度将在原始（未扰动）数据分布的低密度区域产生样本，从而改善分数估计效果&lt;/u&gt;。关键在于，&lt;code&gt;我们训练一个以噪声强度为条件的单一分数网络（score network），并联合估计所有噪声层级下的分数&lt;/code&gt;。进而，&lt;u&gt;我们提出一种退火版本的朗之万动力学：采样初始阶段使用对应最高噪声强度的分数，随后逐步降低噪声强度，直至其小到与原始数据分布难以区分为止&lt;/u&gt;。我们的采样策略受模拟退火（simulated annealing）启发，后者通过启发式方法有效改善了多模态景观下的优化性能。&lt;/p&gt;
&lt;p&gt;我们的方法具备若干理想特性。首先，该目标函数对几乎所有分数网络的参数化形式均易于处理，无需特殊约束或架构设计，且可在训练过程中避免对抗训练、MCMC 采样或其他近似方法。此外，该目标函数还可用于对同一数据集上的不同模型进行定量比较。我们在 MNIST、CelebA 和 CIFAR-10 数据集上通过实验验证了该方法的有效性。结果表明，所生成的样本质量可与现代基于似然的模型及生成对抗网络（GAN）相媲美。在 CIFAR-10 数据集上，我们的模型在无条件生成模型中取得了 8.87 的全新最优 Inception Score，并获得了具有竞争力的 25.32 FID 分数。通过图像修复（inpainting）实验，我们进一步证明该模型能够学习到数据的有意义表征。&lt;/p&gt;
&lt;h3 id=&#34;基于分数的生成模型&#34;&gt;基于分数的生成模型
&lt;/h3&gt;&lt;p&gt;假设我们的数据集由来自未知数据分布 $ p_{\text{data}}(\mathbf{x}) $ 的独立同分布样本 $\{ \mathbf{x}_i \in \mathbb{R}^D \}_{i=1}^N$ 构成。&lt;strong&gt;我们定义概率密度 $ p(\mathbf{x}) $ 的分数（score）为 $ \nabla_{\mathbf{x}} \log p(\mathbf{x}) $&lt;/strong&gt;。分数网络 $ s_\theta : \mathbb{R}^D \to \mathbb{R}^D $ 是一个由参数 $ \theta $ 参数化的神经网络，该网络将被训练以近似 $ p_{\text{data}}(\mathbf{x}) $ 的分数。生成建模的目标是利用数据集学习一个模型，从而从 $ p_{\text{data}}(\mathbf{x}) $ 中生成新的样本。基于分数的生成建模框架包含两个核心组成部分：&lt;span style=&#34;color: red;&#34;&gt;分数匹配&lt;/span&gt; 与 &lt;span style=&#34;color: red;&#34;&gt;朗之万动力学&lt;/span&gt;。&lt;/p&gt;
&lt;h4 id=&#34;分数估计的分数匹配&#34;&gt;分数估计的分数匹配
&lt;/h4&gt;&lt;p&gt;分数匹配（score matching）最初设计用于基于独立同分布样本学习非归一化统计模型。我们将其重新用于分数估计。通过分数匹配，我们可直接训练分数网络 $\mathbf{s}_\theta(\mathbf{x})$ 以估计 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$，无需预先训练模型来估计 $p_{\text{data}}(\mathbf{x})$。与分数匹配的典型用法不同，我们选择不使用基于能量模型的梯度，以避免因高阶导数带来的额外计算开销。该目标函数最小化 $\frac{1}{2} \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \|\mathbf{s}_\theta(\mathbf{x}) - \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})\|_2^2 \right]$，在常数项内等价于以下形式：&lt;/p&gt;
$$
\mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \text{tr}(\nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x})) + \frac{1}{2} \|\mathbf{s}_\theta(\mathbf{x})\|_2^2 \right], \tag{1}
$$&lt;p&gt;其中 $\nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x})$ 表示 $\mathbf{s}_\theta(\mathbf{x})$ 的雅可比矩阵。在一定正则性条件下，式 (1) 的极小值点（记为 $\mathbf{s}_\theta^*(\mathbf{x})$）几乎必然满足 $\mathbf{s}_\theta^*(\mathbf{x}) = \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$。实践中，式 (1) 中关于 $p_{\text{data}}(\mathbf{x})$ 的期望可通过数据样本快速估计。然而，由于 $\text{tr}(\nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}))$ 的计算限制，分数匹配难以扩展至深度网络和高维数据。下文将讨论两种适用于大规模分数匹配的流行方法。&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color: red;&#34;&gt;去噪分数匹配&lt;/span&gt;（Denoising score matching）是分数匹配的一种变体，其核心优势在于&lt;strong&gt;完全规避了雅可比矩阵迹（$\text{tr}(\nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}))$）的计算&lt;/strong&gt;。该方法首先通过预设噪声分布 $q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x})$ 对数据点 $\mathbf{x}$ 施加扰动，随后采用分数匹配技术估计扰动后数据分布 $q_\sigma(\tilde{\mathbf{x}}) \triangleq \int q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) p_{\text{data}}(\mathbf{x}) \mathrm{d}\mathbf{x}$ 的分数。其目标函数经证明等价于：&lt;br&gt;
&lt;/p&gt;
$$
\frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) p_{\text{data}}(\mathbf{x})} \left[ \|\mathbf{s}_\theta(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x})\|_2^2 \right]. \tag{2}
$$&lt;p&gt;最小化式 (2) 的最优分数网络（记为 $\mathbf{s}_\theta^*(\mathbf{x})$）几乎必然满足 $\mathbf{s}_\theta^*(\mathbf{x}) = \nabla_{\mathbf{x}} \log q_\sigma(\mathbf{x})$。然而需注意：&lt;strong&gt;仅当噪声强度足够小（使得 $q_\sigma(\mathbf{x}) \approx p_{\text{data}}(\mathbf{x})$）时&lt;/strong&gt;，才有 $\mathbf{s}_\theta^*(\mathbf{x}) \approx \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$ 成立。&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color: red;&#34;&gt;切片分数匹配&lt;/span&gt;（Sliced score matching）通过随机投影近似分数匹配中的雅可比矩阵迹 $\text{tr}(\nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}))$。其目标函数为：&lt;br&gt;
&lt;/p&gt;
$$
\mathbb{E}_{p_{\mathbf{v}}} \mathbb{E}_{p_{\text{data}}} \left[ \mathbf{v}^\top \nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}) \mathbf{v} + \frac{1}{2} \|\mathbf{s}_\theta(\mathbf{x})\|_2^2 \right], \tag{3}
$$&lt;p&gt;其中 $p_{\mathbf{v}}$ 为简单随机向量分布（例如多变量标准正态分布）。$\mathbf{v}^\top \nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}) \mathbf{v}$ 可通过前向模式自动微分（forward mode auto-differentiation）高效计算。与去噪分数匹配不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;切片分数匹配直接估计&lt;strong&gt;原始未扰动数据分布&lt;/strong&gt;的分数&lt;/li&gt;
&lt;li&gt;但因需前向模式自动微分，其计算量约为去噪分数匹配的四倍&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;关键术语说明&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;英文术语&lt;/th&gt;
          &lt;th&gt;中文译法&lt;/th&gt;
          &lt;th&gt;技术依据&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Sliced score matching&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;切片分数匹配&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;sliced&amp;rdquo; 源自随机投影的&amp;quot;切片&amp;quot;特性（保留数学本质）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;forward mode auto-differentiation&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;前向模式自动微分&lt;/td&gt;
          &lt;td&gt;计算机科学标准译法（对比&amp;quot;反向模式&amp;quot;）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;unperturbed data distribution&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;未扰动数据分布&lt;/td&gt;
          &lt;td&gt;与前文&amp;quot;perturbed data&amp;quot;（扰动后数据）严格对应&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;multivariate standard normal&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;多变量标准正态分布&lt;/td&gt;
          &lt;td&gt;概率论规范表述（避免&amp;quot;多元高斯&amp;quot;口语化）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;基于朗之万动力学的采样&#34;&gt;基于朗之万动力学的采样
&lt;/h4&gt;&lt;p&gt;朗之万动力学（Langevin dynamics）仅需利用分数函数 $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ 即可从概率密度 $p(\mathbf{x})$ 中生成样本。给定固定步长 $\epsilon &gt; 0$ 和初始值 $\tilde{\mathbf{x}}_0 \sim \pi(\mathbf{x})$（其中 $\pi$ 为先验分布），朗之万方法通过以下递归式迭代计算：&lt;/p&gt;
$$
\tilde{\mathbf{x}}_t = \tilde{\mathbf{x}}_{t-1} + \frac{\epsilon}{2} \nabla_{\mathbf{x}} \log p(\tilde{\mathbf{x}}_{t-1}) + \sqrt{\epsilon} \, \mathbf{z}_t, \tag{4}
$$&lt;p&gt;其中 $\mathbf{z}_t \sim \mathcal{N}(0, I)$。当 $\epsilon \to 0$ 且 $T \to \infty$ 时，在满足一定正则性条件下，$\tilde{\mathbf{x}}_T$ 的分布将收敛至 $p(\mathbf{x})$，此时 $\tilde{\mathbf{x}}_T$ 即为 $p(\mathbf{x})$ 的精确样本。当 $\epsilon &gt; 0$ 且 $T &lt; \infty$ 时，需通过 Metropolis-Hastings 更新校正 式 (4) 的误差，但在实际应用中（当 $\epsilon$ 足够小且 $T$ 足够大时），该误差通常可忽略。&lt;/p&gt;
&lt;p&gt;需注意：从式 (4) 采样仅需分数函数 $\nabla_{\mathbf{x}} \log p(\mathbf{x})$。因此，为从 $p_{\text{data}}(\mathbf{x})$ 生成样本，我们可先训练分数网络使 $\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$，再通过 $\mathbf{s}_\theta(\mathbf{x})$ 近似实现朗之万动力学采样。&lt;strong&gt;这正是基于分数的生成建模框架的核心思想&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;基于分数的生成建模所面临的挑战&#34;&gt;基于分数的生成建模所面临的挑战
&lt;/h4&gt;&lt;p&gt;本节将对基于分数的生成建模思想进行更深入的分析。我们认为，存在两大主要障碍，使得该方法无法被直接（朴素地）应用。&lt;/p&gt;
&lt;h5 id=&#34;流形假设&#34;&gt;流形假设
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;流形假设&lt;/strong&gt;指出，&lt;span style=&#34;color: red;&#34;&gt;现实世界的数据往往集中分布在高维空间（即环境空间）中嵌入的低维流形上&lt;/span&gt;。该假设已在诸多数据集中得到经验验证，并成为流形学习的基础理论。在流形假设下，基于分数的生成模型将面临两大关键挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;分数的定义问题&lt;/strong&gt;：由于分数 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$ 是在环境空间中计算的梯度，当数据点 $\mathbf{x}$ 被限制在低维流形上时，该梯度无法良好定义。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分数估计的不一致性&lt;/strong&gt;：分数匹配目标函数（式(1)）仅在数据分布的支撑集为整个空间时提供一致的分数估计器（参见文献 [24] 中的定理 2），而当数据分布支撑于低维流形时，该估计器将变得不一致。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;流形假设对分数估计的负面影响可通过图 1 清晰观察：我们在 CIFAR-10 数据集上训练 ResNet（附录 B.1 详述）以估计数据分数，采用切片分数匹配目标函数（式(3)）实现快速训练与精确估计。如图 1（左）所示，当直接在原始 CIFAR-10 图像上训练时，切片分数匹配损失先下降后出现剧烈波动。相比之下，若对数据施加微小高斯噪声（使扰动后数据分布的支撑集覆盖整个 $\mathbb{R}^D$），损失曲线将稳定收敛（右图）。需注意，我们施加的高斯噪声 $\mathcal{N}(0, 0.0001)$ 对像素值范围在 $[0,1]$ 的图像而言极其微弱，人眼几乎无法辨识。&lt;/p&gt;
&lt;img src=&#34;image-20260211144724709.png&#34; alt=&#34;image-20260211144724709&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;图 1 说明&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;左图&lt;/strong&gt;：无噪声添加时，切片分数匹配（SSM）损失随迭代次数的变化&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;右图&lt;/strong&gt;：添加 $\mathcal{N}(0, 0.0001)$ 高斯噪声后，相同条件下的损失曲线&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;低数据密度区域&#34;&gt;低数据密度区域
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;数据在低密度区域的稀缺性，将同时导致分数匹配与基于朗之万动力学的MCMC采样面临困难。&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;分数匹配中的分数估计不准确性&#34;&gt;分数匹配中的分数估计不准确性
&lt;/h5&gt;&lt;p&gt;在数据密度较低的区域，由于样本数量不足，分数匹配可能无法准确估计分数函数。如第 2.1 节所述，分数匹配的目标是最小化分数估计值的期望平方误差，即 $\frac{1}{2} \mathbb{E}_{p_{\text{data}}} \left[ \|\mathbf{s}_\theta(\mathbf{x}) - \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})\|_2^2 \right]$。在实际应用中，该期望通常通过独立同分布样本 $\{\mathbf{x}_i\}_{i=1}^N \stackrel{\text{i.i.d.}}{\sim} p_{\text{data}}(\mathbf{x})$ 进行估计。考虑任意区域 $\mathcal{R} \subset \mathbb{R}^D$ 满足 $p_{\text{data}}(\mathcal{R}) \approx 0$，在大多数情况下 $\{\mathbf{x}_i\}_{i=1}^N \cap \mathcal{R} = \varnothing$，因此分数匹配无法获得足够样本以准确估计 $\mathcal{R}$ 内 $\mathbf{x}$ 处的 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$。&lt;/p&gt;
&lt;p&gt;为验证此负面影响，我们在图 2 中展示了一个简化实验（附录 B.1 详述）：使用切片分数匹配估计高斯混合分布 $p_{\text{data}} = \frac{1}{5} \mathcal{N}((-5,-5), I) + \frac{4}{5} \mathcal{N}((5,5), I)$ 的分数。如图所示，&lt;strong&gt;分数估计仅在模态（modes）附近的高密度区域内可靠&lt;/strong&gt;，而在低密度区域存在显著偏差。&lt;/p&gt;
&lt;img src=&#34;image-20260211145219245.png&#34; alt=&#34;image-20260211145219245&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;图 2 说明&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;左图&lt;/strong&gt;：真实分数 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$（橙色色阶编码：颜色越深表示密度越高）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;右图&lt;/strong&gt;：估计分数 $\mathbf{s}_\theta(\mathbf{x})$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;红色矩形框&lt;/strong&gt;：标注 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}) \approx \mathbf{s}_\theta(\mathbf{x})$ 的区域&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;langevin-动力学的混合缓慢问题&#34;&gt;Langevin 动力学的混合缓慢问题
&lt;/h5&gt;&lt;p&gt;当数据分布的两个模态被低密度区域分隔时，Langevin 动力学将无法在合理时间内准确恢复这两个模态的相对权重，因此可能无法收敛至真实分布。我们的分析主要受文献 [63] 启发，该文献在分数匹配的密度估计背景下分析了相同现象。&lt;/p&gt;
&lt;p&gt;考虑高斯混合分布 $p_{\text{data}}(\mathbf{x}) = \pi p_1(\mathbf{x}) + (1-\pi)p_2(\mathbf{x})$，其中 $p_1(\mathbf{x})$ 和 $p_2(\mathbf{x})$ 是支撑集不相交的归一化分布，且 $\pi \in (0,1)$。在 $p_1(\mathbf{x})$ 的支撑集内，$\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}) = \nabla_{\mathbf{x}} (\log \pi + \log p_1(\mathbf{x})) = \nabla_{\mathbf{x}} \log p_1(\mathbf{x})$；在 $p_2(\mathbf{x})$ 的支撑集内，$\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}) = \nabla_{\mathbf{x}} (\log(1-\pi) + \log p_2(\mathbf{x})) = \nabla_{\mathbf{x}} \log p_2(\mathbf{x})$。在两种情况下，分数 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$ 均与 $\pi$ 无关。由于 Langevin 动力学使用 $\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$ 从 $p_{\text{data}}(\mathbf{x})$ 采样，所得样本将不依赖于 $\pi$。在实际应用中，当不同模态具有近似不相交的支撑集（可能共享相同支撑但被小密度区域连接）时，该分析结论依然成立。此时 Langevin 动力学理论上可生成正确样本，但可能需要&lt;strong&gt;极小步长&lt;/strong&gt;和&lt;strong&gt;极大步数&lt;/strong&gt;才能完成混合。&lt;/p&gt;
&lt;p&gt;为验证此分析，我们在 3.2.1 节使用的相同高斯混合分布上测试 Langevin 动力学采样，并将结果展示在图 3 中。采样时使用真实分数作为输入。对比图 3(b) 与 (a) 可以明显看出，Langevin 动力学在两个模态间产生的相对密度存在偏差，这与我们的理论分析一致。&lt;/p&gt;
&lt;img src=&#34;image-20260211150255237.png&#34; alt=&#34;image-20260211150255237&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;图 3 说明&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(a)&lt;/strong&gt; 精确采样结果&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(b)&lt;/strong&gt; 使用真实分数的 Langevin 动力学采样结果&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(c)&lt;/strong&gt; 使用退火 Langevin 动力学（annealed Langevin dynamics）的真实分数采样结果&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键结论&lt;/strong&gt;：Langevin 动力学对模态间相对权重的估计存在偏差，而退火 Langevin 动力学能准确恢复相对权重&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;噪声条件分数网络学习与推理&#34;&gt;噪声条件分数网络：学习与推理
&lt;/h3&gt;&lt;p&gt;我们观察到，通过随机高斯噪声扰动数据可使数据分布更适配基于分数的生成建模。首先，由于高斯噪声分布的支撑集覆盖整个空间，扰动后的数据将不再受限于低维流形，从而规避流形假设带来的困难并使分数估计明确定义。其次，较大的高斯噪声能填充原始未扰动数据分布中的低密度区域，使分数匹配获得更充分的训练信号以提升分数估计质量。&lt;strong&gt;更重要的是，通过采用多级噪声，我们可获得一系列收敛至真实数据分布的噪声扰动分布序列&lt;/strong&gt;。我们可借鉴&lt;span style=&#34;color: red;&#34;&gt;模拟退火与退火重要性采样&lt;/span&gt;的思想，利用这些中间分布提升多模态分布下 Langevin 动力学的混合效率。&lt;/p&gt;
&lt;p&gt;基于此洞见，我们提出通过以下两步改进基于分数的生成建模：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;使用多级噪声扰动数据&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;通过训练单一噪声条件分数网络，联合估计所有噪声层级对应的分数&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;训练完成后，在使用 Langevin 动力学生成样本时，我们&lt;strong&gt;初始采用大噪声层级对应的分数，再逐步降低噪声层级&lt;/strong&gt;。这有助于将大噪声层级的优化优势平滑迁移至低噪声层级（此时扰动数据与原始数据几乎无法区分）。下文将详细阐述本方法的技术细节，包括分数网络架构、训练目标函数及 Langevin 动力学的退火调度策略。&lt;/p&gt;
&lt;h4 id=&#34;噪声条件分数网络&#34;&gt;噪声条件分数网络
&lt;/h4&gt;&lt;p&gt;设 $\{\sigma_i\}_{i=1}^L$ 为满足 $\frac{\sigma_1}{\sigma_2} = \dots = \frac{\sigma_{L-1}}{\sigma_L} &gt; 1$ 的正几何序列。定义扰动数据分布 $q_{\sigma}(\mathbf{x}) \triangleq \int p_{\text{data}}(\mathbf{t}) \mathcal{N}(\mathbf{x} \mid \mathbf{t}, \sigma^2 I) \mathrm{d}\mathbf{t}$。我们选择噪声层级 $\{\sigma_i\}_{i=1}^L$，使得 $\sigma_1$ 足够大以缓解第 3 节讨论的困难，而 $\sigma_L$ 足够小以最小化对数据的影响。我们的目标是训练一个噪声条件分数网络，联合估计所有扰动数据分布的分数，即对任意 $\sigma \in \{\sigma_i\}_{i=1}^L$，满足 $\mathbf{s}_\theta(\mathbf{x}, \sigma) \approx \nabla_{\mathbf{x}} \log q_\sigma(\mathbf{x})$。需注意，当 $\mathbf{x} \in \mathbb{R}^D$ 时，$\mathbf{s}_\theta(\mathbf{x}, \sigma) \in \mathbb{R}^D$。&lt;strong&gt;我们将 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$ 称为噪声条件分数网络（Noise Conditional Score Network, NCSN）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;与基于似然的生成模型和 GANs 类似，模型架构设计对生成高质量样本至关重要。在本工作中，我们主要关注适用于图像生成的架构设计，其他领域的架构设计将留待后续研究。由于噪声条件分数网络 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$ 的输出形状与输入图像 $\mathbf{x}$ 相同，我们借鉴语义分割领域成功模型架构的设计思路（例如语义分割任务）。在实验中，我们的模型 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$ &lt;strong&gt;融合了 U-Net 的架构设计&lt;/strong&gt;与&lt;strong&gt;空洞卷积/扩张卷积&lt;/strong&gt;（二者在语义分割中已被证明非常成功）。此外，我们采用&lt;strong&gt;实例归一化（instance normalization）&lt;/strong&gt;，并基于其在部分图像生成任务中的卓越表现，&lt;strong&gt;使用条件实例归一化的改进版本实现对噪声层级 $\sigma_i$ 的条件化&lt;/strong&gt;。关于模型架构的更多细节可详见附录 A。&lt;/p&gt;
&lt;h4 id=&#34;通过分数匹配学习噪声条件分数网络&#34;&gt;通过分数匹配学习噪声条件分数网络
&lt;/h4&gt;&lt;p&gt;切片分数匹配与去噪分数匹配均可用于训练噪声条件分数网络（NCSN）。&lt;strong&gt;我们采用去噪分数匹配，因其在训练速度上略占优势，且更自然地契合噪声扰动数据分布的分数估计任务&lt;/strong&gt;。需强调的是，经验切片分数匹配同样适用于 NCSN 训练。&lt;/p&gt;
&lt;p&gt;我们选择噪声分布为 $q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}} \mid \mathbf{x}, \sigma^2 I)$，因此 $\nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) = -(\tilde{\mathbf{x}} - \mathbf{x}) / \sigma^2$。对于给定噪声强度 $\sigma$，去噪分数匹配的目标函数（式(2)）为：&lt;/p&gt;
$$
\ell(\theta; \sigma) \triangleq \frac{1}{2} \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \mathbb{E}_{\tilde{\mathbf{x}} \sim \mathcal{N}(\mathbf{x}, \sigma^2 I)} \left[ \left\| \mathbf{s}_\theta(\tilde{\mathbf{x}}, \sigma) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^2} \right\|_2^2 \right]. \tag{5}
$$&lt;p&gt;随后，我们将式(5)对所有 $\sigma \in \{\sigma_i\}_{i=1}^L$ 求和，得到统一目标函数：&lt;/p&gt;
$$
\mathcal{L}(\theta; \{\sigma_i\}_{i=1}^L) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \ell(\theta; \sigma_i), \tag{6}
$$&lt;p&gt;其中 $\lambda(\sigma_i) &gt; 0$ 是依赖于 $\sigma_i$ 的系数函数。假设 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$ 具有足够容量，则式(6)的极小值点 $\mathbf{s}_\theta^*(\mathbf{x}, \sigma)$ 满足 $\mathbf{s}_\theta^*(\mathbf{x}, \sigma_i) = \nabla_{\mathbf{x}} \log q_{\sigma_i}(\mathbf{x})$ 几乎必然成立（对所有 $i \in \{1,2,\cdots,L\}$），&lt;strong&gt;因为式(6)是 $L$ 个去噪分数匹配目标函数的锥形组合&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;关于系数函数 $\lambda(\cdot)$ 的取值存在多种可能选择。理想情况下，我们希望所有 $\{\sigma_i\}_{i=1}^L$ 对应的 $\lambda(\sigma_i)\ell(\theta;\sigma_i)$ 具有大致同阶的量级。通过实验观察发现，当分数网络训练至最优时，$\|\mathbf{s}_\theta(\mathbf{x}, \sigma)\|_2 \propto 1/\sigma$。这一现象启发我们选择 $\lambda(\sigma) = \sigma^2$。在此选择下，$\lambda(\sigma)\ell(\theta;\sigma) = \sigma^2 \ell(\theta;\sigma) = \frac{1}{2}\mathbb{E}\left[\|\mathbf{s}_\theta(\tilde{\mathbf{x}}, \sigma) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma}\|_2^2\right]$。由于 $\frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma} \sim \mathcal{N}(0, I)$ 且 $\|\mathbf{s}_\theta(\mathbf{x}, \sigma)\|_2 \propto 1$，可直接推导出 $\lambda(\sigma)\ell(\theta;\sigma)$ 的量级与 $\sigma$ 无关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;需特别强调&lt;/strong&gt;：我们的目标函数（式(6)）具有以下核心优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;训练过程无需对抗训练&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无需代理损失函数&lt;/strong&gt;（例如对比散度等）；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练过程中无需从分数网络进行采样&lt;/strong&gt;（区别于对比散度等方法）；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不要求分数网络具有特殊架构&lt;/strong&gt;即可保证可训练性。&lt;br&gt;
此外，当 $\lambda(\cdot)$ 和 $\{\sigma_i\}_{i=1}^L$ 固定时，该目标函数可用于&lt;strong&gt;定量比较不同噪声条件分数网络（NCSN）的性能&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;通过退火朗之万动力学进行-ncsn-推理&#34;&gt;通过退火朗之万动力学进行 NCSN 推理
&lt;/h4&gt;&lt;p&gt;在训练完噪声条件分数网络 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$ 后，我们提出一种采样方法——&lt;strong&gt;退火朗之万动力学&lt;/strong&gt;（算法 1）——用于生成样本。该方法受&lt;code&gt;模拟退火&lt;/code&gt;和&lt;code&gt;退火重要性采样&lt;/code&gt;的启发。如算法 1 所示，我们首先从某个固定的先验分布（例如均匀噪声）初始化样本 $\tilde{\mathbf{x}}_0$。随后，我们以步长 $\alpha_1$ 运行朗之万动力学，从 $q_{\sigma_1}(\mathbf{x})$ 采样。接着，我们从上一次模拟的最终样本出发，以减小的步长 $\alpha_2$ 运行朗之万动力学，从 $q_{\sigma_2}(\mathbf{x})$ 采样。我们以这种方式继续迭代：将 $q_{\sigma_{i-1}}(\mathbf{x})$ 的朗之万动力学最终样本作为 $q_{\sigma_i}(\mathbf{x})$ 的初始样本，并逐步减小步长 $\alpha_i$（其中 $\alpha_i = \epsilon \cdot \sigma_i^2 / \sigma_L^2$）。最终，我们运行朗之万动力学从 $q_{\sigma_L}(\mathbf{x})$ 采样，当 $\sigma_L \approx 0$ 时，$q_{\sigma_L}(\mathbf{x})$ 与原始数据分布 $p_{\text{data}}(\mathbf{x})$ 高度接近。&lt;/p&gt;
&lt;img src=&#34;image-20260212141759013.png&#34; alt=&#34;image-20260212141759013&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;由于所有扰动分布 $\{q_{\sigma_i}\}_{i=1}^L$ 均通过高斯噪声进行扰动，其支撑集覆盖整个空间且分数定义良好，从而规避了流形假设带来的困难。当 $\sigma_1$ 足够大时，$q_{\sigma_1}(\mathbf{x})$ 的低密度区域显著缩小，且不同模态之间的隔离程度降低。如前所述，这能提升分数估计的准确性并加速朗之万动力学的混合过程。因此，我们可以合理假设朗之万动力学能从 $q_{\sigma_1}(\mathbf{x})$ 生成高质量样本。这些样本很可能处于 $q_{\sigma_1}(\mathbf{x})$ 的高密度区域，这意味着它们&lt;strong&gt;也大概率处于 $q_{\sigma_2}(\mathbf{x})$ 的高密度区域&lt;/strong&gt;——鉴于 $q_{\sigma_1}(\mathbf{x})$ 与 $q_{\sigma_2}(\mathbf{x})$ 仅存在细微差异。由于分数估计和朗之万动力学在高密度区域表现更优，$q_{\sigma_1}(\mathbf{x})$ 的样本可作为 $q_{\sigma_2}(\mathbf{x})$ 朗之万动力学的优质初始样本。同理，$q_{\sigma_{i-1}}(\mathbf{x})$ 为 $q_{\sigma_i}(\mathbf{x})$ 提供优质初始样本，最终我们能从 $q_{\sigma_L}(\mathbf{x})$ 获得高质量样本。&lt;/p&gt;
&lt;p&gt;在算法1中，关于步长 $\alpha_i$ 的调整策略存在多种可能方案。我们的选择是 $\alpha_i \propto \sigma_i^2$，其动机在于&lt;strong&gt;固定朗之万动力学中的&amp;quot;信号-噪声比&amp;quot; $\frac{\alpha_i \mathbf{s}_\theta(\mathbf{x}, \sigma_i)}{2\sqrt{\alpha_i} \mathbf{z}}$ 的量级&lt;/strong&gt;。需注意：$\mathbb{E}\left[\left\|\frac{\alpha_i \mathbf{s}_\theta(\mathbf{x}, \sigma_i)}{2\sqrt{\alpha_i} \mathbf{z}}\right\|_2^2\right] \approx \mathbb{E}\left[\frac{\alpha_i \|\mathbf{s}_\theta(\mathbf{x}, \sigma_i)\|_2^2}{4}\right] \propto \frac{1}{4} \mathbb{E}\left[\|\mathbf{s}_\theta(\mathbf{x}, \sigma_i)\|_2^2\right]$。如前所述，当分数网络训练接近最优时，$\|\mathbf{s}_\theta(\mathbf{x}, \sigma_i)\|_2 \propto 1/\sigma_i$，因此 $\left\|\frac{\alpha_i \mathbf{s}_\theta(\mathbf{x}, \sigma_i)}{2\sqrt{\alpha_i} \mathbf{z}}\right\|_2^2 \propto \frac{1}{4} \mathbb{E}\left[\|\mathbf{s}_\theta(\mathbf{x}, \sigma_i)\|_2^2\right] \propto \frac{1}{4}$，&lt;strong&gt;该量级与 $\sigma_i$ 无关&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为验证退火朗之万动力学的有效性，我们通过一个玩具实验进行演示：仅使用分数从两个分离良好的高斯混合分布中采样（实验设置同3.2节）。我们采用算法1进行采样，选择 $\{\sigma_i\}_{i=1}^L$ 为几何级数，其中 $L=10$，$\sigma_1=10$，$\sigma_{10}=0.1$。实验结果展示于图3。对比图3(b)与(c)可见，&lt;strong&gt;退火朗之万动力学能正确恢复两模态间的相对权重，而标准朗之万动力学则失败&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;实验&#34;&gt;实验
&lt;/h3&gt;&lt;h3 id=&#34;相关工作&#34;&gt;相关工作
&lt;/h3&gt;&lt;p&gt;我们的方法与若干通过学习马尔可夫链转移算子实现采样的方法存在相似性。例如，生成随机网络（GSN）利用去噪自编码器训练一个马尔可夫链，其平衡分布与数据分布相匹配。类似地，我们的方法通过训练朗之万动力学中使用的分数函数，从数据分布中进行采样。但需注意：GSN 通常从接近训练数据点的位置初始化马尔可夫链，因此要求链在不同模态间快速转移。相比之下，我们的退火朗之万动力学从非结构化噪声初始化。非平衡热力学（NET）采用预设扩散过程将数据缓慢转换为随机噪声，再通过训练逆向扩散过程实现还原。然而，NET 可扩展性较差，因其要求扩散过程步长极小，且训练时需模拟数千步的马尔可夫链。&lt;/p&gt;
&lt;p&gt;先前的方法（如扩散训练（IT）和变分回溯（VW））同样采用不同噪声层级/温度训练马尔可夫链的转移算子。IT 和 VW（以及 NET）均通过最大化适当边缘似然的证据下界（ELBO）训练模型。实践中，它们常生成模糊的图像样本（与变分自编码器类似）。&lt;strong&gt;与之形成鲜明对比的是，我们的目标函数基于分数匹配而非似然优化，因此能够生成与 GAN 相媲美的高质量图像&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;与前述方法相比，我们的方法存在若干结构性差异。首先，&lt;strong&gt;我们不需要在训练过程中从马尔可夫链采样&lt;/strong&gt;。相比之下，生成随机网络（GSN）的回溯过程需要多次运行马尔可夫链以生成&amp;quot;负样本&amp;quot;。包括非平衡热力学（NET）、扩散训练（IT）和变分回溯（VW）在内的其他方法，也需要为每个输入模拟马尔可夫链来计算训练损失。这种差异使我们的方法在训练深度模型时更加高效且可扩展。其次，&lt;strong&gt;我们的训练与采样方法相互解耦&lt;/strong&gt;。对于分数估计，可采用切片分数匹配和去噪分数匹配等任意方法；对于采样，可使用任何基于分数的方法，包括朗之万动力学和（潜在的）哈密顿蒙特卡洛。我们的框架允许任意组合分数估计器和（基于梯度的）采样方法，而大多数先前方法则将模型与特定的马尔可夫链绑定。最后，&lt;strong&gt;我们的方法可用于训练基于能量的模型（EBM）&lt;/strong&gt;，只需将基于能量模型的梯度作为分数模型。相比之下，目前尚不清楚先前学习马尔可夫链转移算子的方法如何直接用于训练 EBM。&lt;/p&gt;
&lt;p&gt;分数匹配最初是为学习基于能量的模型（EBM）而提出的。然而，许多现有基于分数匹配的方法要么不可扩展，要么无法产生与变分自编码器（VAE）或生成对抗网络（GAN）相当质量的样本。为提升深度基于能量模型的训练性能，一些近期工作转向对比散度，并提出在训练和测试中使用朗之万动力学进行采样。然而，与我们的方法不同，对比散度在训练中使用计算昂贵的朗之万动力学作为内循环。将退火与去噪分数匹配相结合的思想也在先前工作中在不同背景下进行了研究。然而，这些工作主要聚焦于学习表示以改进分类性能，而非生成建模。去噪分数匹配的方法也可从贝叶斯最小二乘的角度推导，并可借助 Stein 无偏风险估计器的技术实现。&lt;/p&gt;
&lt;h3 id=&#34;结论&#34;&gt;结论
&lt;/h3&gt;&lt;p&gt;我们提出了一种基于分数的生成建模框架：首先通过分数匹配估计数据密度的梯度，再利用朗之万动力学生成样本。我们分析了该方法朴素应用所面临的若干挑战，并提出通过训练&lt;strong&gt;噪声条件分数网络（NCSN）&lt;/strong&gt; 和采用&lt;strong&gt;退火朗之万动力学采样&lt;/strong&gt;来应对这些挑战。本方法无需对抗训练、训练过程中无需 MCMC 采样，且对模型架构无特殊要求。实验表明，我们的方法能够生成此前仅由最优基于似然模型和 GAN 生成的高质量图像。我们在 CIFAR-10 数据集上取得了&lt;strong&gt;8.87 的最先进 Inception Score&lt;/strong&gt;，且 FID 分数与 SNGANs 相当。&lt;/p&gt;
&lt;h3 id=&#34;a-架构&#34;&gt;A 架构
&lt;/h3&gt;&lt;p&gt;本实验中所用噪声条件分数网络（NCSN）的架构包含三个重要组件：&lt;strong&gt;实例归一化&lt;/strong&gt;、&lt;strong&gt;空洞卷积&lt;/strong&gt;和&lt;strong&gt;U-Net类型架构&lt;/strong&gt;。下文将详细介绍这些组件的背景，并阐述我们如何修改它们以适应研究需求。若需更全面的细节和参考实现，建议读者查阅我们公开的代码库。我们的分数网络基于 &lt;strong&gt;PyTorch&lt;/strong&gt; 实现，代码 &lt;a class=&#34;link&#34; href=&#34;https://github.com/ermongroup/ncsn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;a1-实例归一化&#34;&gt;A.1 实例归一化
&lt;/h3&gt;&lt;p&gt;我们采用条件实例归一化，使分数网络 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$ 在预测分数时能够考虑噪声水平 $\sigma$。在条件实例归一化中，针对不同的 $\sigma \in \{\sigma_i\}_{i=1}^L$ 采用不同的缩放参数与偏置参数。具体而言，假设输入 $\mathbf{x}$ 包含 $C$ 个特征图。令 $\mu_k$ 和 $s_k$ 分别表示 $\mathbf{x}$ 的第 $k$ 个特征图沿空间轴计算的均值与标准差。条件实例归一化通过以下公式实现：&lt;/p&gt;
$$
\mathbf{z}_k = \gamma[i,k] \frac{\mathbf{x}_k - \mu_k}{s_k} + \beta[i,k],
$$&lt;p&gt;其中 $\gamma \in \mathbb{R}^{L \times C}$ 和 $\beta \in \mathbb{R}^{L \times C}$ 为可学习参数，$k$ 表示特征图索引，$i$ 表示 $\{\sigma_i\}_{i=1}^L$ 中 $\sigma$ 的索引。&lt;/p&gt;
&lt;p&gt;然而，实例归一化的一个缺点是它完全移除了不同特征图的 $\mu_k$ 信息，这可能导致生成图像出现颜色偏移。为解决此问题，我们提出对条件实例归一化进行简单改进：首先计算 $\mu_k$ 的均值与标准差，分别记为 $m$ 和 $v$；然后引入另一个可学习参数 $\alpha \in \mathbb{R}^{L \times C}$。改进后的条件实例归一化定义为：&lt;/p&gt;
$$
\mathbf{z}_k = \gamma[i,k] \frac{\mathbf{x}_k - \mu_k}{s_k} + \beta[i,k] + \alpha[i,k] \frac{\mu_k - m}{v}.
$$&lt;p&gt;我们将这种改进的条件实例归一化简称为 &lt;strong&gt;CondInstanceNorm++&lt;/strong&gt;。在我们的架构中，&lt;strong&gt;CondInstanceNorm++ 被添加在每个卷积层和池化层之前&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;a2-空洞卷积&#34;&gt;A.2 空洞卷积
&lt;/h3&gt;&lt;p&gt;空洞卷积可在保持特征图分辨率的同时扩大感受野。研究表明，空洞卷积在语义分割任务中效果显著，因其能更好地保留高分辨率特征图中的位置信息。在噪声条件分数网络（NCSN）的架构设计中，&lt;strong&gt;我们使用空洞卷积替换除第一层外的所有下采样层&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;a3-u-net-架构&#34;&gt;A.3 U-Net 架构
&lt;/h3&gt;&lt;p&gt;U-Net 是一种具有特殊跳跃连接的架构。这些跳跃连接有助于将浅层中的低层信息（如位置和形状）传递至网络的深层。由于浅层通常包含位置和形状等低层信息，这些跳跃连接有助于提升语义分割的结果。为构建 $\mathbf{s}_\theta(\mathbf{x}, \sigma)$，我们采用 RefineNet 的架构，这是一种结合了 ResNet 设计的 U-Net 现代变体。关于 RefineNet 架构的详细描述，建议读者参考文献 [32]。&lt;/p&gt;
&lt;p&gt;在我们的实验中，&lt;strong&gt;我们使用 4 级级联的 RefineNet&lt;/strong&gt;。我们采用预激活残差块（pre-activation residual blocks），并移除 RefineNet 架构中的所有批归一化层，替换为 CondInstanceNorm++。我们将 RefineNet 中的&lt;strong&gt;最大池化层替换为平均池化&lt;/strong&gt;，因为平均池化在图像生成任务（如风格迁移）中被证实能产生更平滑的图像。此外，我们在 RefineNet 块中的每个卷积和平均池化前添加 CondInstanceNorm++（尽管原始 RefineNet 块中未使用归一化）。所有激活函数均选择为 ELU。如前所述，我们&lt;strong&gt;使用空洞卷积替换残差块中的下采样层（第一层除外）&lt;/strong&gt;。遵循常规实践，我们在进入下一级联时将膨胀率增加一倍。对于 CelebA 和 CIFAR-10 实验，第一级对应的层的滤波器数量为 128，其他级的滤波器数量加倍；而对于 MNIST 实验，滤波器数量则减半。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion</title>
        <link>https://ruheyun.github.io/p/diffusion/</link>
        <pubDate>Thu, 29 Jan 2026 15:55:20 +0800</pubDate>
        
        <guid>https://ruheyun.github.io/p/diffusion/</guid>
        <description>&lt;h1 id=&#34;扩散模型笔记1&#34;&gt;扩散模型笔记(1)
&lt;/h1&gt;&lt;h2 id=&#34;论文基本信息&#34;&gt;论文基本信息
&lt;/h2&gt;&lt;p&gt;&lt;mark&gt;论文名称&lt;/mark&gt;：Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;出版期刊&lt;/mark&gt;：ICML 2015&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;学校机构&lt;/mark&gt;：Stanford University，University of California, Berkeley&lt;/p&gt;
&lt;h2 id=&#34;论文翻译&#34;&gt;论文翻译
&lt;/h2&gt;&lt;p&gt;以下翻译结合了AI工具和我对论文的理解，在翻译的过程中会省略我认为不重要的部分，并不是全篇一字一句的翻译。并且可能翻译或理解有误。（此过程同时借助了&lt;a class=&#34;link&#34; href=&#34;https://chatgpt.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatGPT&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://chat.qwen.ai/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Qwen&lt;/a&gt;两个AI工具）&lt;/p&gt;
&lt;h3 id=&#34;标题&#34;&gt;标题
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;利用非平衡热力学的深度无监督学习&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;摘要&#34;&gt;摘要
&lt;/h3&gt;&lt;p&gt;机器学习中的一个核心问题在于：&lt;u&gt;如何利用高度灵活的概率分布族对复杂数据集进行建模，同时保证学习、采样、推断和评估在解析上或计算上仍具有可行性。&lt;/u&gt;本文提出了一种能够同时实现灵活性与可处理性的方法。&lt;mark&gt;其核心思想受非平衡统计物理学启发，即通过迭代式的前向扩散过程，系统且缓慢地破坏数据分布中的结构。随后，我们学习一个反向扩散过程以恢复数据中的结构，从而得到一个兼具高度灵活性与可处理性的数据生成模型。&lt;/mark&gt;该方法使我们能够快速地在具有数千层或时间步的深度生成模型中进行学习、采样与概率评估，并可计算所学模型下的条件概率与后验概率。此外，我们还开源发布了该算法的参考实现&lt;a class=&#34;link&#34; href=&#34;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;引言&#34;&gt;引言
&lt;/h3&gt;&lt;p&gt;历史上，概率模型一直面临两个相互冲突目标之间的权衡：可处理性与灵活性。具有可处理性的模型能够进行解析式评估并易于拟合数据（例如高斯分布或拉普拉斯分布）。然而，这些模型难以恰当地描述丰富数据集中的复杂结构。另一方面，灵活的模型则可以被建模以拟合任意数据中的结构。例如，我们可以通过任意（非负）函数 $\phi(x)$ 来定义模型，从而得到灵活的分布 $p(x) = \frac{\phi(x)}{Z}$，其中 Z 为归一化常数。然而，计算该归一化常数通常是不可行的。对这类灵活模型进行概率评估、训练或采样通常需要代价高昂的&lt;strong&gt;蒙特卡洛过程&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;目前已存在多种解析近似方法，它们虽可缓解但无法彻底消除这一权衡——例如平均场理论及其扩展、变分贝叶斯、对比散度、最小概率流、最小 KL 收缩、恰当评分规则、分数匹配、伪似然、循环信念传播等等，不一而足。此外，&lt;code&gt;非参数方法&lt;/code&gt;亦可取得显著成效。&lt;/p&gt;
&lt;p&gt;非参数方法可被视为在可处理模型与灵活模型之间实现平滑过渡的一种途径。例如，非参数高斯混合模型在表示少量数据时可能仅使用单个高斯分布，而在表示无限量数据时则可扩展为由无限多个高斯分布构成的混合模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参数化 GMM：需预先设定 &lt;code&gt;K=5&lt;/code&gt; 个高斯分量，无论数据多复杂都只能用这 5 个分量拟合；&lt;/p&gt;
&lt;p&gt;非参数 GMM：无需指定 &lt;code&gt;K&lt;/code&gt;，算法自动学习需要多少个分量——简单数据可能只用 2 个分量，复杂数据可能用 20 个甚至更多。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;扩散概率模型&#34;&gt;扩散概率模型
&lt;/h4&gt;&lt;p&gt;我们提出了一种定义概率模型的新方法，该方法具备以下特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;模型结构具有极高的灵活性；&lt;/li&gt;
&lt;li&gt;支持精确采样；&lt;/li&gt;
&lt;li&gt;易于与其他分布相乘（例如用于计算后验概率）；&lt;/li&gt;
&lt;li&gt;模型的对数似然以及单个状态的概率均可高效计算。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们的方法利用马尔可夫链逐步将一个分布转换为另一个分布，这一思想源于&lt;mark&gt;非平衡统计物理学&lt;/mark&gt;和&lt;mark&gt;序列蒙特卡洛方法&lt;/mark&gt;。我们构建了一个生成式马尔可夫链，通过扩散过程将一个简单的已知分布（例如高斯分布）转换为目标（数据）分布。与使用马尔可夫链对已预先定义的模型进行近似评估不同，我们&lt;strong&gt;显式地将概率模型定义为马尔可夫链的终点&lt;/strong&gt;。由于扩散链中的每一步都具有可解析计算的概率，因此整个链的概率亦可进行解析式评估。&lt;/p&gt;
&lt;p&gt;在该框架下的学习过程涉及对扩散过程中的微小扰动进行估计。相比使用单一的、无法解析归一化的&lt;mark&gt;势函数&lt;/mark&gt;来显式描述完整分布，估计微小扰动在计算上更为可行。此外，由于对任意光滑的目标分布都存在对应的扩散过程，该方法能够捕捉任意形式的数据分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Potential function&lt;/strong&gt;：势函数，源自物理学&amp;quot;势能&amp;quot;（potential energy）概念，在概率模型中表示未归一化的能量/权重函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们通过在多个数据集上训练高对数似然模型，验证了这些扩散概率模型的有效性。所用数据集包括：二维瑞士卷（two-dimensional swiss roll）、二值序列、手写数字（MNIST）以及若干自然图像数据集（CIFAR-10、树皮纹理和枯叶模型）。&lt;/p&gt;
&lt;h4 id=&#34;相关的其他工作&#34;&gt;相关的其他工作
&lt;/h4&gt;&lt;p&gt;wake-sleep algorithm 首次提出了让推断模型与生成概率模型相互对抗训练的思想。此后近二十年间，这一思路在很大程度上未被深入探索，尽管存在一些例外工作。近年来，围绕该思想的研究呈现爆发式增长。之后提出的变分学习与推断算法，使得灵活的生成模型与潜在变量的后验分布能够被直接联合训练、相互优化。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;有些算法翻译为中文很不好理解，所以不做翻译。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这些论文中使用的变分界与我们训练目标中采用的变分界以及 Sminchisescu 等人（2006）早期工作中的形式相似。然而，我们的动机与模型形式均存在显著差异。相较于上述方法，本文工作保留了以下区别与优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们借鉴物理学、准静态过程（quasi-static processes）以及退火重要性采样（annealed importance sampling）的思想构建框架，而非基于变分贝叶斯方法；&lt;/li&gt;
&lt;li&gt;我们展示了如何便捷地将所学分布与另一概率分布相乘（例如与条件分布相乘以计算后验分布）；&lt;/li&gt;
&lt;li&gt;我们解决了变分推断方法中推断模型训练尤为困难的问题——该困难源于推断模型与生成模型在目标函数中的不对称性。为此，我们将前向（推断）过程限制为简单的函数形式，使得反向（生成）过程具有相同的函数形式；&lt;/li&gt;
&lt;li&gt;我们训练了包含数千层（或时间步）的模型，而非仅限于少量层次；&lt;/li&gt;
&lt;li&gt;我们为每一层（或时间步）中的熵产生（entropy production）提供了上界与下界估计。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;存在多种与&lt;code&gt;概率模型&lt;/code&gt;训练相关的技术（下文将简要总结），这些方法致力于构建高度灵活的生成模型形式、训练随机轨迹，或学习贝叶斯网络的逆过程。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;技术/作者&lt;/th&gt;
          &lt;th&gt;时间&lt;/th&gt;
          &lt;th&gt;简述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Reweighted wake-sleep&lt;/td&gt;
          &lt;td&gt;2015&lt;/td&gt;
          &lt;td&gt;对原始wake-sleep algorithm提出了扩展并改进了学习规则。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Generative stochastic networks&lt;/td&gt;
          &lt;td&gt;2013/2014&lt;/td&gt;
          &lt;td&gt;通过训练马尔可夫核，使其平稳分布与数据分布相匹配。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Neural autoregressive distribution estimators&lt;/td&gt;
          &lt;td&gt;2011/2013&lt;/td&gt;
          &lt;td&gt;将联合分布分解为一系列关于各维度的可处理条件分布序列。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Adversarial networks&lt;/td&gt;
          &lt;td&gt;2014&lt;/td&gt;
          &lt;td&gt;通过让生成模型与一个试图区分生成样本与真实数据的分类器进行对抗训练。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schmidhuber（作者）&lt;/td&gt;
          &lt;td&gt;1992&lt;/td&gt;
          &lt;td&gt;提出的类似目标学习了一个双向映射，将数据映射到具有边缘独立单元的表示空间。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Rippel 与 Adams以及 Dinh（作者）&lt;/td&gt;
          &lt;td&gt;2013/2014&lt;/td&gt;
          &lt;td&gt;学习了到潜在表示空间的双射确定性映射，该潜在空间具有简单的阶乘密度函数。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Stuhlmüller（作者）&lt;/td&gt;
          &lt;td&gt;2013&lt;/td&gt;
          &lt;td&gt;贝叶斯网络学习了随机逆映射。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mixtures of conditional Gaussian scale mixtures, MCGSMs&lt;/td&gt;
          &lt;td&gt;2012&lt;/td&gt;
          &lt;td&gt;利用高斯尺度混合来描述数据集，其参数依赖于一系列因果邻域。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此外，还有大量工作致力于学习从简单潜在分布到数据分布的灵活生成映射——早期范例包括 MacKay 将神经网络引入作为生成模型，以及 Bishop 等学习从潜在空间到数据空间的随机流形映射。&lt;code&gt;本文将在实验中与对抗网络及 MCGSMs 进行对比。&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;来自物理学的相关思想包括 &lt;strong&gt;Jarzynski 等式&lt;/strong&gt;，该等式在机器学习中被称为&lt;strong&gt;退火重要性采样&lt;/strong&gt;（Annealed Importance Sampling, AIS）。AIS 利用马尔可夫链将一个分布缓慢转换为另一个分布，以计算归一化常数的比值。Burda 等人进一步证明，AIS 亦可基于反向轨迹而非前向轨迹执行。&lt;strong&gt;朗之万动力学&lt;/strong&gt;（Langevin dynamics）作为福克-普朗克方程（Fokker-Planck equation）的随机实现形式，展示了如何定义一个高斯扩散过程，使其以任意目标分布作为平稳分布。福克-普朗克方程也被用来进行随机优化。最后，&lt;strong&gt;柯尔莫哥洛夫前向与后向方程&lt;/strong&gt;（Kolmogorov forward and backward equations）表明：对于许多前向扩散过程，其对应的反向扩散过程可用相同的函数形式加以描述。&lt;/p&gt;
&lt;h3 id=&#34;算法&#34;&gt;算法
&lt;/h3&gt;&lt;p&gt;我们的目标是定义一个前向（或推断）扩散过程，将任意复杂的数据分布转换为简单且易于处理的分布；随后学习该扩散过程的有限时间反向过程，以此定义我们的生成模型分布（见图1）。我们首先描述前向推断扩散过程，继而展示如何训练反向生成扩散过程并利用其进行概率评估。此外，我们还为反向过程推导了熵的上下界，并说明所学分布如何与任意第二个分布相乘（例如在图像修复或去噪任务中计算后验分布时所需的操作）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ruheyun.github.io/p/diffusion/image-20260130142618684.png&#34;
	width=&#34;832&#34;
	height=&#34;606&#34;
	srcset=&#34;https://ruheyun.github.io/p/diffusion/image-20260130142618684_hu_ea920d4b659a5180.png 480w, https://ruheyun.github.io/p/diffusion/image-20260130142618684_hu_8f132b3a25a13647.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;图1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;137&#34;
		data-flex-basis=&#34;329px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;图1：在 2-d swiss roll data 上训练的建模框架。第一行展示了前向轨迹 $q(x^{(0:T)})$ 在各时间切片上的分布。数据分布（左）经历高斯扩散过程，逐步转化为单位协方差高斯分布（右）。第二行展示了训练所得反向轨迹 $p(x^{(0:T)})$ 对应的时间切片。单位协方差高斯分布（右）经过具有学习得到的均值与协方差函数的高斯扩散过程，逐步重构回原始数据分布（左）。第三行展示了同一反向扩散过程的漂移项（drift term）$f_\mu(x^{(t)}, t) - x^{(t)}$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p style=&#34;color: red;&#34;&gt;为什么论文这里又称前向过程为 Inference Process？&lt;/p&gt;
&lt;p&gt;在这篇论文中，“inference” 指的是：给定一个真实数据样本 x(0) ，推断（或构造）它在整个扩散轨迹中对应的噪声版本序列 x(1),x(2),…,x(T) 。这里的 “inference” 不是指“从训练好的模型生成新样本”，而是指“为训练目的，从已知数据反推其在扩散链中的演化路径”。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;前向轨迹&#34;&gt;前向轨迹
&lt;/h4&gt;&lt;p&gt;我们将数据分布记为 $q(x^{(0)})$。通过反复应用针对 $\pi(y)$ 的马尔可夫扩散核 $T_\pi(y \mid y&#39;; \beta)$，数据分布被逐步转换为一个性质良好（解析可处理）的分布 $\pi(y)$，其中 $\beta$ 为扩散速率，
&lt;/p&gt;
$$
\pi (y) = \int dy&#39; \, T_\pi (y|y&#39;; \beta) \, \pi (y&#39;) \tag{1}
$$$$
q \left(x^{(t)} \mid x^{(t-1)} \right) = T_\pi \left(x^{(t)} \mid x^{(t-1)}; \beta_t \right). \tag{2}
$$&lt;blockquote&gt;
&lt;p&gt;公式 (1)：目标分布的平稳性条件。&lt;/p&gt;
&lt;p&gt;该公式定义了 目标分布 $\pi (y)$ 作为扩散过程的平稳分布（equilibrium distribution）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$T_{\pi}(y∣y′;\beta)$ 是 &lt;strong&gt;马尔可夫扩散核&lt;/strong&gt;（Markov diffusion kernel），表示从状态 $y′$ 到 $y$ 的转移概率，$\beta$ 是&lt;strong&gt;扩散速率&lt;/strong&gt;（diffusion rate）。&lt;/li&gt;
&lt;li&gt;$\pi(y)$ 满足：当系统达到平衡时，应用扩散核 $T_{\pi}$ 后分布保持不变。&lt;/li&gt;
&lt;li&gt;在扩散模型中，$\pi(y)$ 通常选择为&lt;strong&gt;标准高斯分布&lt;/strong&gt;（如 $\pi(y) = N(y;0,I)$），因其解析性质良好（易于计算）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;公式 (2)：前向扩散过程的定义。&lt;/p&gt;
&lt;p&gt;该公式定义了 前向扩散过程（inference process）的条件转移概率：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$q(x^{(t)}∣x^{(t−1)})$ 是在时间步 $t−1$ 给定 $x^{(t−1)}$ 时，状态 $x^{(t)}$ 的条件分布。&lt;/li&gt;
&lt;li&gt;$T_{\pi}(x^{(t)}∣x^{(t−1)};β_t)$ 是&lt;strong&gt;时间相关的扩散核&lt;/strong&gt;，$\beta_t$ 是第 $t$ 步的&lt;strong&gt;扩散速率&lt;/strong&gt;（通常随时间递增）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：前向过程使用与目标分布 $\pi (y)$ 相同的扩散核 $T_{\pi}$，但 $\pi$ 随时间变化（$\beta_t$ 而非固定 $\beta$）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;前向轨迹对应于从数据分布开始并执行 $T$ 步扩散的过程，因此有
&lt;/p&gt;
$$
q \left( x^{(0 \cdots T)} \right) = q \left( x^{(0)} \right) \prod_{t=1}^T q \left( x^{(t)} \mid x^{(t-1)} \right) \tag{3}
$$&lt;blockquote&gt;
&lt;p&gt;马尔科夫性质，每一步只依赖与前一步，与其他步独立。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在下文展示的实验中，$q(x^{(t)}∣x^{(t−1)})$ 对应于以下两种扩散过程之一：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;高斯扩散&lt;/strong&gt;：将数据逐步扩散至单位协方差高斯分布（identity-covariance Gaussian distribution）；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;二项扩散&lt;/strong&gt;：将数据逐步扩散至独立二项分布（independent binomial distribution）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;反向轨迹&#34;&gt;反向轨迹
&lt;/h4&gt;&lt;p&gt;生成分布将被训练以描述相同的轨迹，但方向相反，
&lt;/p&gt;
$$
p \left( x^{(T)} \right) = \pi \left( x^{(T)} \right) \tag{4}
$$$$
p \left( x^{(0 \cdots T)} \right) = p \left( x^{(T)} \right) \prod_{t=1}^T p \left( x^{(t-1)} \mid x^{(t)} \right). \tag{5}
$$&lt;blockquote&gt;
&lt;p&gt;反向轨迹也是马尔科夫链，根据马尔科夫性质得到的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键意义&lt;/strong&gt;：该框架通过学习反向扩散轨迹，使模型能够从简单分布 $\pi(x^{(T)})$ 逐步还原数据分布 $q(x^{(0)})$。由于每一步反向转移概率 $p(x^{(t−1)}∣x^{(t)})$ 均可参数化为神经网络，因此整个生成过程既灵活又可解析计算。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于高斯扩散与二项扩散，在连续扩散情形下（即步长 $\beta$ 趋近于零的极限），扩散过程的反向过程具有与前向过程&lt;strong&gt;完全相同的函数形式&lt;/strong&gt;。由于 $q(x^{(t)}∣x^{(t−1)})$ 服从高斯（或二项）分布，且当 $\beta_t$ 足够小时，反向条件分布 $q(x^{(t−1)}∣x^{(t)})$ 同样服从高斯（或二项）分布。轨迹长度 $T$ 越长，每一步的扩散速率 $\beta$ 即可取得越小，从而使离散扩散过程更逼近连续极限，反向过程的函数形式亦更精确地匹配前向过程。&lt;/p&gt;
&lt;p&gt;在学习过程中，仅需估计高斯扩散核的&lt;strong&gt;均值与协方差&lt;/strong&gt;，或二项扩散核的&lt;strong&gt;比特翻转概率&lt;/strong&gt;。如表 App.1 所示，$f_{\mu}(x^{(t)},t)$ 与 $f_{\Sigma}(x^{(t)},t)$ 是定义高斯分布反向马尔可夫转移的均值与协方差的函数，$f_b(x^{(t)},t)$ 是为二项分布提供比特翻转概率的函数。运行该算法的计算成本等于这些函数的计算开销乘以时间步数。本文所有实验结果均采用多层感知机（MLP）来参数化这些函数。然而，大量回归或函数拟合技术均可适用，包括非参数方法。&lt;/p&gt;
&lt;h3 id=&#34;模型概率&#34;&gt;模型概率
&lt;/h3&gt;&lt;p&gt;生成模型赋予原始数据的概率为
&lt;/p&gt;
$$
p \left( x^{(0)} \right) = \int dx^{(1 \cdots T)} \, p \left( x^{(0 \cdots T)} \right). \tag{6}
$$&lt;p&gt;
直观上，该积分难以处理——但借鉴退火重要性采样（annealed importance sampling）和 Jarzynski 等式的思想，我们转而评估前向与反向轨迹的相对概率，并对前向轨迹进行平均，
&lt;/p&gt;
$$
\begin{align}
p \left( x^{(0)} \right) &amp;= \int dx^{(1 \cdots T)} \, p \left( x^{(0 \cdots T)} \right) \frac{q \left( x^{(1 \cdots T)} \mid x^{(0)} \right)}{q \left( x^{(1 \cdots T)} \mid x^{(0)} \right)} \tag{7} \\
&amp;= \int dx^{(1 \cdots T)} \, q \left( x^{(1 \cdots T)} \mid x^{(0)} \right) \frac{p \left( x^{(0 \cdots T)} \right)}{q \left( x^{(1 \cdots T)} \mid x^{(0)} \right)} \tag{8} \\
&amp;= \int dx^{(1 \cdots T)} \, q \left( x^{(1 \cdots T)} \mid x^{(0)} \right) p \left( x^{(T)} \right) \prod_{t=1}^{T} \frac{p \left( x^{(t-1)} \mid x^{(t)} \right)}{q \left( x^{(t)} \mid x^{(t-1)} \right)}. \tag{9}
\end{align}
$$&lt;blockquote&gt;
&lt;p&gt;公式（7）：概率表达式的重写，分子分母同乘相同因子，等式不变；&lt;/p&gt;
&lt;p&gt;公式（8）：重要性采样形式，将积分改写为对前向轨迹的&lt;strong&gt;期望值&lt;/strong&gt;；&lt;/p&gt;
&lt;p&gt;公式（9）：轨迹相对概率的显示表达，由将（5）带入（8）得到。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;该积分可通过从前向轨迹 $q(x^{(1⋯T)}∣x^{(0)})$ 中采样并取平均值来快速评估。当扩散步长 $\beta$ 趋近于无穷小时，前向与反向轨迹分布可趋于一致。若二者完全相同，则仅需从前向轨迹 $q(x^{(1⋯T)}∣x^{(0)})$ 中采样&lt;strong&gt;单一样本&lt;/strong&gt;，即可通过代入精确计算上述积分。此情形对应于统计物理学中的&lt;strong&gt;准静态过程&lt;/strong&gt;（quasi-static process）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;核心概念解析&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;概念&lt;/th&gt;
          &lt;th&gt;说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;快速评估&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过蒙特卡洛采样估计积分 $E_q[⋅]$，避免高维解析积分&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;轨迹分布一致&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;当 $\beta \rightarrow 0$ 且 $T \rightarrow \infty$（连续扩散极限）时，$p(x^{(0⋯T)})=q(x^{(0⋯T)})$，此时相对概率比值恒为 1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;单样本精确估计&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;若 $p=q$，则 $\frac{p(x^{(0⋯T)})}{q(x^{(1⋯T)}∣x^{(0)})}=const$，任意单样本即可给出精确期望值（方差为零）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;准静态过程&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;热力学中系统在无限缓慢变化下始终保持平衡态的过程；在扩散模型中，对应于每步扰动极小、前向/反向过程可逆的理想极限&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;实践意义&lt;/strong&gt;：虽然实际应用中 $\beta$ 无法取无穷小（计算成本限制），但该理论表明——&lt;strong&gt;轨迹越长、每步扰动越小，模型对数似然的估计方差越低&lt;/strong&gt;。这为扩散模型设计提供了理论指导：增加时间步数 $T$ 可提升概率估计精度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;训练&#34;&gt;训练
&lt;/h3&gt;&lt;p&gt;训练等价于最大化模型的对数似然，
&lt;/p&gt;
$$
L = \int dx^{(0)} q\left(x^{(0)}\right) \log p\left(x^{(0)}\right) \tag{10}
$$&lt;p&gt;
该目标函数可以重写为：
&lt;/p&gt;
$$
L = \int dx^{(0)} q\left(x^{(0)}\right) \log \left[ \int dx^{(1 \cdots T)} q\left(x^{(1 \cdots T)} \mid x^{(0)}\right) p\left(x^{(T)}\right) \prod_{t=1}^T \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)} \right], \tag{11}
$$&lt;p&gt;
该表达式由&lt;code&gt;Jensen不等式&lt;/code&gt;提供了一个下界：
&lt;/p&gt;
$$
L \geq \int dx^{(0 \cdots T)} q\left(x^{(0 \cdots T)}\right) \log \left[ p\left(x^{(T)}\right) \prod_{t=1}^T \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)} \right]. \tag{12}
$$&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;公式解析（q不含参数，p含参数）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;公式（10）：最大似然目标。本质是 期望对数似然（负交叉熵）。&lt;/p&gt;
&lt;p&gt;公式（11）：将公式（9）带入（10），通过引入重要性采样，将高维积分转化为轨迹期望。&lt;/p&gt;
&lt;p&gt;公式（12）：Jensen 不等式下界，避免计算轨迹比值，下界可表示为单轨迹的对数似然，通过蒙特卡洛采样估计。&lt;/p&gt;
&lt;p&gt;该是扩散模型的训练目标函数。它将复杂的最大似然估计转化为一个可优化的目标，且：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无需计算归一化常数&lt;/li&gt;
&lt;li&gt;仅需对轨迹进行采样和局部计算&lt;/li&gt;
&lt;li&gt;与变分推断中的 ELBO 本质相同，但通过扩散过程结构实现了更精确的下界&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;与单样本精确估计的关联&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当模型完美学习（$p=q$）且处于准静态极限时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;轨迹比值 $\frac{p(x^{(0:T)})}{q(x^{(1:T)}∣x^{(0)})}$ 变为&lt;strong&gt;常数&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;此时 $log⁡E[X]=E[log⁡X]$，不等式取等，下界达到紧致&lt;/li&gt;
&lt;li&gt;这解释了为何在理想情况下&amp;quot;单样本即可精确估计&amp;quot;（但实际中需多采样）&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;如附录 B 所述，对于我们的扩散轨迹，该下界简化为，
&lt;/p&gt;
$$
L \geq K \tag{13}
$$$$
\begin{align*}
K = &amp;-\sum_{t=2}^T \int dx^{(0)} dx^{(t)} q\left(x^{(0)}, x^{(t)}\right) D_{KL}\left(q\left(x^{(t-1)} \mid x^{(t)}, x^{(0)}\right) \parallel p\left(x^{(t-1)} \mid x^{(t)}\right)\right)  \\
&amp;+ H_q\left(X^{(T)} \mid X^{(0)}\right) - H_q\left(X^{(1)} \mid X^{(0)}\right) - H_p\left(X^{(T)}\right). \tag{14}
\end{align*}
$$&lt;blockquote&gt;
&lt;p&gt;公式（14）求和项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本质是每步的 KL 散度期望&lt;/li&gt;
&lt;li&gt;衡量反向生成过程 p 与前向推断过程 q 的差异&lt;/li&gt;
&lt;li&gt;训练目标：最小化该差异（即让 p 尽可能接近 q 的逆过程）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;熵项：&lt;/p&gt;
&lt;p&gt;$H_q\left(X^{(T)} \mid X^{(0)}\right)$：最终状态 $X^{(T)}$ 在给定初始状态 $X^{(0)}$ 下的条件熵&lt;/p&gt;
&lt;p&gt;$H_q\left(X^{(1)} \mid X^{(0)}\right)$：第一步后的条件熵&lt;/p&gt;
&lt;p&gt;$H_p\left(X^{(T)}\right)$：模型在最终状态 $X^{(T)}$ 的熵（通常为 0，因为 $X^{(T)} \sim \pi$ 是固定分布）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中熵和 KL 散度均可解析计算。该下界的推导与&lt;strong&gt;变分贝叶斯方法&lt;/strong&gt;中对数似然界（log likelihood bound）的推导过程平行。&lt;/p&gt;
&lt;p&gt;训练过程包括寻找能够最大化对数似然下界的反向马尔可夫转移概率，
&lt;/p&gt;
$$
\hat{p}\left(x^{(t-1)} \mid x^{(t)}\right) = \underset{p\left(x^{(t-1)} \mid x^{(t)}\right)}{\operatorname{argmax}} K. \tag{15}
$$&lt;p&gt;
由此，&lt;strong&gt;概率分布的估计问题被简化为对函数的回归任务&lt;/strong&gt;：这些函数用于设定一系列高斯分布的均值与协方差（或设定一系列伯努利试验的状态翻转概率）。&lt;/p&gt;
&lt;h4 id=&#34;选择扩散比率&#34;&gt;选择扩散比率$\beta_t$
&lt;/h4&gt;&lt;p&gt;前向轨迹中 $\beta_t$ 的选择对训练后模型的性能至关重要。在退火重要性采样（AIS）中，中间分布的合理调度可显著提升对数配分函数估计的准确性。在热力学中，系统在不同平衡分布之间转移时所采用的路径调度，决定了自由能的损耗程度。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;自由能损耗：非平衡过程中因不可逆性导致的能量耗散。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在高斯扩散的情形下，我们通过在 $K$ 上执行梯度上升来学习前向扩散调度 $\beta_{2 \cdots T}$。第一步的方差 $\beta_1$ 被固定为一个较小的常数，以防止过拟合。为了显式表达从 $q(x^{(1 \cdots T)} \mid x^{(0)})$ 中采样的样本对 $\beta_{1 \cdots T}$ 的依赖关系，我们采用&amp;quot;冻结噪声&amp;quot;（frozen noise）技术——如（Kingma 与 Welling，2013）所述，将噪声视为额外的辅助变量，并在计算 $K$ 对参数的偏导数时保持其恒定。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\beta$ 调度的选择有两种：固定或可学习。这段描述的是可学习的清况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;即使在可学习调度中，$\beta_1$ 仍被固定为小常数（防止过拟合）&lt;/li&gt;
&lt;li&gt;可学习的是 $\beta_2$ 到 $\beta_T$，即从第 2 步开始的扩散速率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“冻结噪声”技术：这是&lt;strong&gt;使采样过程对 $\beta_t$ 可微的关键技巧&lt;/strong&gt;，本质与 VAE 中的&amp;quot;重参数化技巧&amp;quot;相同。&lt;/p&gt;
&lt;p&gt;刚开始以为和调度的选择有关，然后发现这个技术是&lt;strong&gt;为了使随机过程可微，&amp;ldquo;冻结&amp;quot;是指计算梯度时将已采样的噪声视为常数&lt;/strong&gt;，是扩散模型中的必要技术，其实就是&amp;quot;重参数化技巧&amp;quot;的另一个表达形式。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于二项扩散，离散状态空间使得基于冻结噪声的梯度上升方法无法实现（因离散变量不可微）。我们转而选择前向扩散调度 $\beta_{1 \cdots T}$，使得每一步扩散擦除原始信号的固定比例 $1/T$，由此得到的扩散速率为 $\beta_t = (T - t + 1)^{-1}$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在扩散模型里（无论 Gaussian 还是 binomial），都会用到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\beta_t$：第 t 步的噪声强度&lt;/li&gt;
&lt;li&gt;$1-\beta_t$：第 t 步信号保留比例&lt;/li&gt;
&lt;li&gt;累积信号保留量：$\bar{\alpha}_t = \prod_{s=1}^{t}(1-\beta_s)$ （表示第 t 步之后，还剩多少“原始信号”）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在二项扩散情况下，每步抹掉 $\frac{1}{T}$ 原始信1号：&lt;/p&gt;
&lt;p&gt;则 $\bar{\alpha}_t = 1 - \frac{t}{T}$&lt;/p&gt;
&lt;p&gt;由 $\bar{\alpha}_t = \bar{\alpha}_{t-1}(1-\beta_t)$&lt;/p&gt;
&lt;p&gt;可解得 $\beta_t = \frac{1}{T-t+1}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;分布相乘与后验计算&#34;&gt;分布相乘与后验计算
&lt;/h3&gt;&lt;p&gt;如信号去噪或缺失值推断等任务需要计算后验分布，这要求将模型分布 $p(x^{(0)})$ 与第二个分布（或有界正函数）$r(x^{(0)})$ 相乘，从而生成一个新的分布 $\tilde{p}(x^{(0)}) \propto p(x^{(0)}) \, r(x^{(0)})$。&lt;/p&gt;
&lt;p&gt;对于许多技术而言，分布相乘在计算上代价高昂且实现困难，这些技术包括变分自编码器（VAE）、生成式随机网络（GSN）、神经自回归分布估计器（NADE）以及大多数图模型。然而，在扩散模型框架下，分布相乘却十分直接：第二个分布 $r(x^{(0)})$ 既可以被视为对扩散过程中每一步的微小扰动，也常常能够精确地融入每一步扩散操作中。后续章节将详细阐述如何在扩散概率模型的框架下实现分布相乘。&lt;/p&gt;
&lt;h4 id=&#34;修改后的边际分布&#34;&gt;修改后的边际分布
&lt;/h4&gt;&lt;p&gt;首先，为了计算 $\tilde{p}(x^{(0)})$，我们将每个中间分布乘以一个对应的函数 $r(x^{(t)})$。我们使用波浪号（tilde）表示该分布或马尔可夫转移属于一条经过此类修改的轨迹。$\tilde{p}(x^{(0\cdots T)})$ 是修改后的反向轨迹，它从分布 $\tilde{p}(x^{(T)}) = \frac{1}{\tilde{Z}_T} p(x^{(T)}) r(x^{(T)})$ 开始，并通过一系列修改后的中间分布进行传递：&lt;/p&gt;
$$
\tilde{p}\left(x^{(t)}\right) = \frac{1}{\tilde{Z}_t} p\left(x^{(t)}\right) r\left(x^{(t)}\right), \tag{16}
$$&lt;p&gt;其中 $\tilde{Z}_t$ 是第 $t$ 个中间分布的归一化常数。&lt;/p&gt;
&lt;h4 id=&#34;修改后的扩散步骤&#34;&gt;修改后的扩散步骤
&lt;/h4&gt;&lt;p&gt;逆扩散过程的马尔可夫核 $p(x^{(t)} \mid x^{(t+1)})$ 满足平衡条件：&lt;/p&gt;
$$
p(x^{(t)}) = \int dx^{(t+1)} \, p(x^{(t)} \mid x^{(t+1)}) \, p(x^{(t+1)}). \tag{17}
$$&lt;p&gt;
我们希望扰动的马尔可夫核 $\tilde{p}(x^{(t)} \mid x^{(t+1)})$ 满足扰动分布的平衡条件，&lt;/p&gt;
$$
\tilde{p}(x^{(t)}) = \int dx^{(t+1)} \, \tilde{p}(x^{(t)} \mid x^{(t+1)}) \, \tilde{p}(x^{(t+1)}), \tag{18}
$$$$
\frac{p(x^{(t)}) \, r(x^{(t)})}{\tilde{Z}_t} = \int dx^{(t+1)} \, \frac{\tilde{p}(x^{(t)} \mid x^{(t+1)}) \, p(x^{t+1}) \, r(x^{(t+1)})}{\tilde{Z}_{t+1}}, \tag{19}
$$$$
p(x^{(t)}) = \int dx^{(t+1)} \, \tilde{p}(x^{(t)} \mid x^{(t+1)}) \, \frac{\tilde{Z}_t \, r(x^{(t+1)})}{\tilde{Z}_{t+1} \, r(x^{(t)})} \, p(x^{(t+1)}). \tag{20}
$$&lt;p&gt;若满足&lt;/p&gt;
$$
\tilde{p}\left(x^{(t)} \mid x^{(t+1)}\right) = p\left(x^{(t)} \mid x^{(t+1)}\right) \frac{\tilde{Z}_{t+1} r\left(x^{(t)}\right)}{\tilde{Z}_t r\left(x^{(t+1)}\right)}. \tag{21}
$$&lt;p&gt;则方程 (20) 成立。&lt;/p&gt;
&lt;p&gt;方程 (21) 可能不对应于一个归一化的概率分布，因此我们选择 $\tilde{p}(x^{(t)} \mid x^{(t+1)})$ 为相应的归一化分布：&lt;/p&gt;
$$
\tilde{p}\left(x^{(t)} \mid x^{(t+1)}\right) = \frac{1}{\tilde{Z}_t\left(x^{(t+1)}\right)} p\left(x^{(t)} \mid x^{(t+1)}\right) r\left(x^{(t)}\right), \tag{22}
$$&lt;p&gt;其中 $\tilde{Z}_t(x^{(t+1)})$ 是归一化常数。&lt;/p&gt;
&lt;p&gt;对于高斯分布，由于其方差较小，每一步扩散通常相对于 $r(x^{(t)})$ 具有非常尖锐的峰值。这意味着 $\frac{r(x^{(t)})}{r(x^{(t+1)})}$ 可视为对 $p(x^{(t)} \mid x^{(t+1)})$ 的微小扰动。对高斯分布的微小扰动仅影响其均值，而不会改变归一化常数，因此在此情况下方程 (21) 与 (22) 是等价的（见附录 C）。&lt;/p&gt;
&lt;h4 id=&#34;应用&#34;&gt;应用 $r(x^{(t)})$
&lt;/h4&gt;&lt;p&gt;若 $r(x^{(t)})$ 足够平滑，则可将其视为对反向扩散核 $p(x^{(t)} \mid x^{(t+1)})$ 的微小扰动。在此情况下，$\tilde{p}(x^{(t)} \mid x^{(t+1)})$ 将具有与 $p(x^{(t)} \mid x^{(t+1)})$ 相同的函数形式，但对于高斯核而言其均值将被扰动，而对于二项核而言其翻转率将被扰动。扰动后的扩散核见表 App.1，并在附录 C 中针对高斯核进行了推导。&lt;/p&gt;
&lt;p&gt;若 $r(x^{(t)})$ 可以与高斯（或二项）分布以闭式形式相乘，则它可以直接与反向扩散核 $p(x^{(t)} \mid x^{(t+1)})$ 以闭式形式相乘。这种情况适用于 $r(x^{(t)})$ 由某些坐标子集的 delta 函数构成的情形，例如图 5 中的图像修复（inpainting）示例。&lt;/p&gt;
&lt;h4 id=&#34;选择&#34;&gt;选择 $r(x^{(t)})$
&lt;/h4&gt;&lt;p&gt;通常，$r(x^{(t)})$ 应当在整个轨迹过程中&lt;strong&gt;缓慢变化&lt;/strong&gt;。在本文的实验中，我们选择其为常数，&lt;/p&gt;
$$
r\left(x^{(t)}\right) = r\left(x^{(0)}\right). \tag{23}
$$&lt;p&gt;
另一种便捷的选择是 $r(x^{(t)}) = r(x^{(0)})^{\frac{T-t}{T}}$。在此第二种选择下，$r(x^{(t)})$ 对反向轨迹的起始分布没有贡献。这保证了从 $\tilde{p}(x^{(T)})$ 中抽取反向轨迹的初始样本仍然简单直接。&lt;/p&gt;
&lt;h3 id=&#34;反向过程的熵&#34;&gt;反向过程的熵
&lt;/h3&gt;&lt;p&gt;由于前向过程已知，我们可以推导反向轨迹中每一步条件熵的上下界，从而得到对数似然的上下界，公式 24：&lt;/p&gt;
$$
\begin{align*}
H_q\left(X^{(t)} \mid X^{(t-1)}\right) + H_q\left(X^{(t-1)} \mid X^{(0)}\right) - H_q\left(X^{(t)} \mid X^{(0)}\right) &amp;\leq H_q\left(X^{(t-1)} \mid X^{(t)}\right) \\
&amp;\leq H_q\left(X^{(t)} \mid X^{(t-1)}\right), \tag{24}
\end{align*}
$$&lt;p&gt;其中上下界仅依赖于 $q(x^{(1 \cdots T)} \mid x^{(0)})$，并且可以解析计算。推导过程见附录 A。&lt;/p&gt;
&lt;h3 id=&#34;实验&#34;&gt;实验
&lt;/h3&gt;&lt;p&gt;使用的是 Theano 库，现在已淘汰，学术上更多的用 pytorch，看代码时能看懂思路就行。&lt;/p&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;我们提出了一种新颖的概率分布建模算法，该算法支持精确采样与概率评估，并在多种玩具数据集与真实数据集（包括具有挑战性的自然图像数据集）上验证了其有效性。在所有实验中，我们均采用相似的基础算法，表明该方法能够准确建模广泛多样的分布形态。&lt;/p&gt;
&lt;p&gt;现有大多数密度估计技术为保持计算可行性与效率，往往不得不牺牲建模能力，且采样或概率评估的计算成本通常极高。本算法的核心在于&lt;strong&gt;估计一个马尔可夫扩散链的逆过程&lt;/strong&gt;——该扩散链将数据映射至噪声分布。随着扩散步数的增加，每一步的逆分布逐渐变得简单且易于估计。最终得到的算法兼具以下优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能够拟合任意数据分布（高建模灵活性）&lt;/li&gt;
&lt;li&gt;训练过程可处理（tractable）&lt;/li&gt;
&lt;li&gt;支持精确采样与概率评估&lt;/li&gt;
&lt;li&gt;便于对条件分布与后验分布进行直接操作&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这一设计在建模能力与计算效率之间实现了前所未有的平衡。&lt;/p&gt;
&lt;h3 id=&#34;附录&#34;&gt;附录
&lt;/h3&gt;&lt;h4 id=&#34;a-条件熵边界推导&#34;&gt;A 条件熵边界推导
&lt;/h4&gt;&lt;p&gt;反向轨迹中一步的条件熵 $H_q(X^{(t−1)}∣X^{(t)})$ 为
&lt;/p&gt;
$$
H_q\left(X^{(t-1)}, X^{(t)}\right) = H_q\left(X^{(t)}, X^{(t-1)}\right) \tag{25}
$$$$
H_q\left(X^{(t-1)} \mid X^{(t)}\right) + H_q\left(X^{(t)}\right) = H_q\left(X^{(t)} \mid X^{(t-1)}\right) + H_q\left(X^{(t-1)}\right) \tag{26}
$$$$
H_q\left(X^{(t-1)} \mid X^{(t)}\right) = H_q\left(X^{(t)} \mid X^{(t-1)}\right) + H_q\left(X^{(t-1)}\right) - H_q\left(X^{(t)}\right) \tag{27}
$$&lt;blockquote&gt;
&lt;p&gt;公式 25 ：联合熵的对称性&lt;/p&gt;
&lt;p&gt;公式 26 ：条件熵之间的关系&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过观察 $\pi(y)$ 是最大熵分布，可以构建熵变化的上界。这一结论对二项分布无条件成立，对高斯分布的情形则要求训练数据的方差为 1。对于高斯分布的情况，因此需要将训练数据缩放到单位范数以保证下述等式成立。但不需要进行白化处理。该上界推导如下，
&lt;/p&gt;
$$
H_q\left(X^{(t)}\right) \geq H_q\left(X^{(t-1)}\right) \tag{28}
$$$$
H_q\left(X^{(t-1)}\right) - H_q\left(X^{(t)}\right) \leq 0 \tag{29}
$$$$
H_q\left(X^{(t-1)} \mid X^{(t)}\right) \leq H_q\left(X^{(t)} \mid X^{(t-1)}\right). \tag{30}
$$&lt;blockquote&gt;
&lt;p&gt;公式 28 ：随着扩散步数增加，数据分布的熵单调非减&lt;/p&gt;
&lt;p&gt;公式 30 ：将 27 代入 29 得到，表明了反向过程的条件熵不超过前向过程的条件熵，即&lt;strong&gt;给出了反向过程条件熵的紧致上界&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过观察马尔可夫链中的额外步骤不会增加关于链中初始状态的可用信息，因此不会降低初始状态的条件熵，可以建立熵差的下界，
&lt;/p&gt;
$$
H_q\left(X^{(0)} \mid X^{(t)}\right) \geq H_q\left(X^{(0)} \mid X^{(t-1)}\right) \tag{31}
$$$$
\begin{align*}
H_q\left(X^{(t-1)}\right) - H_q\left(X^{(t)}\right) \geq \,&amp; H_q\left(X^{(0)} \mid X^{(t-1)}\right) + H_q\left(X^{(t-1)}\right) \\
&amp;- H_q\left(X^{(0)} \mid X^{(t)}\right) - H_q\left(X^{(t)}\right) 
\tag{32}
\end{align*}
$$$$
H_q\left(X^{(t-1)}\right) - H_q\left(X^{(t)}\right) \geq H_q\left(X^{(0)}, X^{(t-1)}\right) - H_q\left(X^{(0)}, X^{(t)}\right) \tag{33}
$$$$
H_q\left(X^{(t-1)}\right) - H_q\left(X^{(t)}\right) \geq H_q\left(X^{(t-1)} \mid X^{(0)}\right) - H_q\left(X^{(t)} \mid X^{(0)}\right) \tag{34}
$$$$
H_q\left(X^{(t-1)} \mid X^{(t)}\right) \geq H_q\left(X^{(t)} \mid X^{(t-1)}\right) + H_q\left(X^{(t-1)} \mid X^{(0)}\right) - H_q\left(X^{(t)} \mid X^{(0)}\right). \tag{35}
$$&lt;blockquote&gt;
&lt;p&gt;公式 31 ：随着扩散步数增加，关于初始状态 $X^{(0)}$ 的条件熵单调非减&lt;/p&gt;
&lt;p&gt;公式 32-35 ：熵差下界的推导，得到单步条件熵下界&lt;/p&gt;
&lt;p&gt;公式 35 ：将 27 代入 34 可得&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;结合上述不等式，我们可以得到&lt;strong&gt;单步条件熵的紧致边界&lt;/strong&gt;
&lt;/p&gt;
$$
\begin{align*}
H_q\left(X^{(t)} \mid X^{(t-1)}\right) &amp;\geq H_q\left(X^{(t-1)} \mid X^{(t)}\right) \\ &amp;\geq H_q\left(X^{(t)} \mid X^{(t-1)}\right) + H_q\left(X^{(t-1)} \mid X^{(0)}\right) - H_q\left(X^{(t)} \mid X^{(0)}\right). \tag{36}
\end{align*}
$$&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;仅依赖前向轨迹&lt;/strong&gt;：上下界均只依赖于已知的前向扩散过程 $q(x^{(1⋯T)}∣x^{(0)})$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解析可计算&lt;/strong&gt;：无需蒙特卡洛采样，可直接通过概率分布的解析形式计算&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;紧致性&lt;/strong&gt;：在准静态极限下（$\beta \rightarrow0$），上下界收敛，实现精确计算&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;b-对数似然下界&#34;&gt;B. 对数似然下界
&lt;/h4&gt;&lt;p&gt;对数似然的下界为&lt;/p&gt;
$$
L \geq K \tag{37}
$$$$
K = \int dx^{(0 \cdots T)} q\left(x^{(0 \cdots T)}\right) \log \left[ p\left(x^{(T)}\right) \prod_{t=1}^T \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)} \right] \tag{38}
$$&lt;blockquote&gt;
&lt;p&gt;这里对应正文公式 （11）（12）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;b1--的熵&#34;&gt;B.1. $p(X^{(T)})$ 的熵
&lt;/h4&gt;&lt;p&gt;我们可以分离出 $p(X^{(T)})$ 的贡献，并将其重写为熵，
&lt;/p&gt;
$$
K = \int dx^{(0 \cdots T)} \, q\left(x^{(0 \cdots T)}\right) \sum_{t=1}^T \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)} \right] + \int dx^{(T)} \, q\left(x^{(T)}\right) \log p\left(x^{(T)}\right) \tag{40}
$$$$
= \int dx^{(0\cdots T)} q\left(x^{(0\cdots T)}\right) \sum_{t=1}^T \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)} \right] + \int dx^{(T)} q\left(x^{(T)}\right) \log \pi\left(x^{(T)}\right) \tag{41}
$$&lt;p&gt;根据设计（选择或规范化了核与参数，使得每一步的二阶矩保持与 $\pi$ 相同），相对于 $\pi(x^{(t)})$ 的交叉熵在我们的扩散核下是常数，并且等于 $p(x^{(T)})$ 的熵。&lt;/p&gt;
&lt;p&gt;因此，&lt;/p&gt;
$$
K = \sum_{t=1}^T \int dx^{(0\cdots T)} q\left(x^{(0\cdots T)}\right) \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)} \right] - H_q\left(X^{(T)}\right). \tag{43}
$$&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;最大熵性质&lt;/strong&gt;：在给定方差的约束下，高斯分布具有&lt;strong&gt;最大熵&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;交叉熵恒定&lt;/strong&gt;：若 $q(x^{(t)})$ 与 $\pi (x^{(t)})$ 具有&lt;strong&gt;相同的二阶矩&lt;/strong&gt;（方差），则：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
$$
-\int q\left(x^{(t)}\right) \log \pi\left(x^{(t)}\right) \, dx^{(t)} 
= -\int \pi\left(x^{(t)}\right) \log \pi\left(x^{(t)}\right) \, dx^{(t)} 
= H(\pi)
$$&lt;blockquote&gt;
&lt;p&gt;即：任何具有单位方差的分布与标准高斯 $\pi$ 的交叉熵都等于 $\pi$ 的熵&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特别地&lt;/strong&gt;：当 $t=T$ 时，$q(x^{(T)})=\pi (x^{(T)})$，此时交叉熵退化为 $\pi$ 的熵：&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
$$
\int q\left(x^{(T)}\right) \log \pi\left(x^{(T)}\right) \, dx^{(T)} 
= \int \pi\left(x^{(T)}\right) \log \pi\left(x^{(T)}\right) \,    dx^{(T)} 
= -H(\pi) = -H_p\left(X^{(T)}\right)
$$&lt;blockquote&gt;
&lt;p&gt;只要 $q(x^{(t)})$ 与 $\pi (x^{(t)})$ 具有相同的二阶矩，交叉熵即恒等于 $\pi$ 的熵，与 $q$ 的具体形式无关。这是扩散模型设计中利用高斯分布最大熵性质的关键技巧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心推导&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设目标分布为标准高斯 $\pi(x)=N(x;0,I)$，其概率密度为：&lt;/p&gt;
&lt;/blockquote&gt;
$$
\pi(x) = (2\pi)^{-d/2} \exp\left(-\frac{1}{2}x^\top x\right)
$$&lt;blockquote&gt;
&lt;p&gt;对数形式为：&lt;/p&gt;
&lt;/blockquote&gt;
$$
\log \pi(x) = -\frac{d}{2} \log(2\pi) - \frac{1}{2}x^\top x
$$&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;关键观察&lt;/strong&gt;：$log⁡\pi(x)$ 仅包含常数项和 $x^⊤x$（即二阶矩）。&lt;/p&gt;
&lt;p&gt;交叉熵定义为：&lt;/p&gt;
&lt;/blockquote&gt;
$$
H(q, \pi) = -\int q(x) \log \pi(x) \, dx
$$&lt;blockquote&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;/blockquote&gt;
$$
\begin{aligned}
H(q, \pi) &amp;= -\int q(x) \left[-\frac{d}{2} \log(2\pi) - \frac{1}{2}x^\top x\right] dx \\
&amp;= \frac{d}{2} \log(2\pi) \underbrace{\int q(x)dx}_{=1} + \frac{1}{2} \int q(x)x^\top x \, dx \\
&amp;= \frac{d}{2} \log(2\pi) + \frac{1}{2} \mathbb{E}_q[x^\top x]
\end{aligned}
$$&lt;blockquote&gt;
&lt;p&gt;同理，$\pi$ 的熵为：&lt;/p&gt;
&lt;/blockquote&gt;
$$
H(\pi) = \frac{d}{2} \log(2\pi) + \frac{1}{2} \mathbb{E}_{\pi}[x^T x]
$$&lt;blockquote&gt;
&lt;p&gt;因此，当 $E_q[x^⊤x]=E_{\pi}[x^⊤x]$ 时：交叉熵退化为熵&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;b2-在-t0-时移除边缘效应&#34;&gt;B.2. 在 t=0 时移除边缘效应
&lt;/h4&gt;&lt;p&gt;为了避免边缘效应，我们将反向轨迹的最后一步设置为与对应的前向扩散步骤相同，
&lt;/p&gt;
$$
p\left(x^{(0)} \mid x^{(1)}\right) = q\left(x^{(1)} \mid x^{(0)}\right) \frac{\pi\left(x^{(0)}\right)}{\pi\left(x^{(1)}\right)} = T_{\pi}\left(x^{(0)} \mid x^{(1)}; \beta_1\right). \tag{44}
$$&lt;p&gt;
然后我们利用这种等价性来移除求和中第一步的贡献，
&lt;/p&gt;
$$
\begin{align*}
K =  &amp;\sum_{t=2}^{T} \int dx^{(0\cdots T)} q\left(x^{(0\cdots T)}\right) \log \left[\frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)}\right] \\
&amp;+ \int dx^{(0)} dx^{(1)} q\left(x^{(0)}, x^{(1)}\right) \log \left[\frac{q\left(x^{(1)} \mid x^{(0)}\right) \pi\left(x^{(0)}\right)}{q\left(x^{(1)} \mid x^{(0)}\right) \pi\left(x^{(1)}\right)}\right] - H_p\left(X^{(T)}\right) \tag{45}
\end{align*}
$$$$
K = \sum_{t=2}^{T} \int dx^{(0\cdots T)} q\left(x^{(0\cdots T)}\right) \log \left[\frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}\right)}\right] - H_p\left(X^{(T)}\right), \tag{46}
$$&lt;p&gt;
其中我们再次利用了这样一个事实：根据设计，$−∫dx^{(t)}q(x^{(t)})log⁡\pi(x^{(t)})=H_p(X^{(T)})$ 对所有 $t$ 都是一个常数。&lt;/p&gt;
&lt;h4 id=&#34;b3-重写后验分布-qxt1x0&#34;&gt;B.3. 重写后验分布 q(x(t−1)∣x(0))
&lt;/h4&gt;&lt;p&gt;由于前向轨迹是一个马尔可夫过程，
&lt;/p&gt;
$$
K = \sum_{t=2}^{T} \int dx^{(0 \cdots T)} \, q\left(x^{(0 \cdots T)}\right) \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}, x^{(0)}\right)} \right] - H_p\left(X^{(T)}\right). \tag{47}
$$&lt;p&gt;
利用贝叶斯规则将条件概率重写为后验分布形式，
&lt;/p&gt;
$$
K = \sum_{t=2}^{T} \int dx^{(0 \cdots T)} \, q\left(x^{(0 \cdots T)}\right) \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}, x^{(0)}\right)} \cdot \frac{q\left(x^{(t-1)} \mid x^{(0)}\right)}{q\left(x^{(t)} \mid x^{(0)}\right)} \right] - H_p\left(X^{(T)}\right). \tag{48}
$$&lt;h4 id=&#34;b4-用kl散度和熵重写&#34;&gt;B.4. 用KL散度和熵重写
&lt;/h4&gt;&lt;p&gt;然后我们认识到一些项是条件熵，
&lt;/p&gt;
$$
\begin{align*}
K = &amp;\sum_{t=2}^{T} \int dx^{(0 \cdots T)} \, q\left(x^{(0 \cdots T)}\right) \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}, x^{(0)}\right)} \right] \\
&amp;+ \sum_{t=2}^{T} \left[ H_q\left(X^{(t)} \mid X^{(0)}\right) - H_q\left(X^{(t-1)} \mid X^{(0)}\right) \right] - H_p\left(X^{(T)}\right) \tag{49}
\end{align*}
$$$$
\begin{align*}
K = &amp;\sum_{t=2}^{T} \int dx^{(0 \cdots T)} \, q\left(x^{(0 \cdots T)}\right) \log \left[ \frac{p\left(x^{(t-1)} \mid x^{(t)}\right)}{q\left(x^{(t)} \mid x^{(t-1)}, x^{(0)}\right)} \right] \\
&amp;+ H_q\left(X^{(T)} \mid X^{(0)}\right) - H_q\left(X^{(1)} \mid X^{(0)}\right) - H_p\left(X^{(T)}\right) \tag{50}
\end{align*}
$$&lt;blockquote&gt;
&lt;p&gt;通过**识别条件熵（公式 49）&lt;strong&gt;并利用&lt;/strong&gt;望远镜级数（公式 50）**的特性，将复杂的求和项简化为仅依赖于初始和最终状态的表达式。这是扩散模型能够高效计算对数似然下界的关键数学技巧。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后，我们将概率分布的对数比转换为KL散度，
&lt;/p&gt;
$$
\begin{align*}
K = &amp;-\sum_{t=2}^{T} \int dx^{(0)} dx^{(t)} q\left(x^{(0)}, x^{(t)}\right) D_{KL}\left(q\left(x^{(t-1)} \mid x^{(t)}, x^{(0)}\right) \parallel p\left(x^{(t-1)} \mid x^{(t)}\right)\right) \\
&amp;+ H_q\left(X^{(T)} \mid X^{(0)}\right) - H_q\left(X^{(1)} \mid X^{(0)}\right) - H_p\left(X^{(T)}\right). \tag{51}
\end{align*}
$$&lt;p&gt;
注意：熵项可以解析计算，且给定 $x^{(0)}$ 和 $x^{(t)}$ 时，KL 散度也可以解析计算。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该公式将复杂的概率计算问题转化为&lt;strong&gt;可解析计算的KL散度&lt;/strong&gt;和&lt;strong&gt;熵项&lt;/strong&gt;。由于扩散过程的前向轨迹已知，所有这些项都可以&lt;strong&gt;精确计算&lt;/strong&gt;，无需蒙特卡洛采样。这是扩散模型区别于其他生成模型（如GAN、VAE）的核心优势。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;c-扰动高斯转移&#34;&gt;C. 扰动高斯转移
&lt;/h4&gt;&lt;p&gt;我们希望计算 $\tilde{p}(x^{(t−1)}∣x^{(t)})$。为简化符号表示，令 $\mu=f_{\mu}(x^{(t)},t)$，$\Sigma =f_{\Sigma}(x^{(t)},t)$，且 $y=x(t−1)$。使用此符号表示，
&lt;/p&gt;
$$
\tilde{p}(y \mid x^{(t)}) \propto p(y \mid x^{(t)}) r(y) 
= \mathcal{N}(y; \mu, \Sigma) r(y). \tag{53}
$$&lt;p&gt;
我们可以将此重写为能量函数的形式，其中 $E_r(y)=−log⁡r(y)$，
&lt;/p&gt;
$$
\tilde{p}(y \mid x^{(t)}) \propto \exp\left[-E(y)\right] \tag{54}
$$$$
E(y) = \frac{1}{2}(y - \mu)^T \Sigma^{-1} (y - \mu) + E_r(y). \tag{55}
$$&lt;blockquote&gt;
&lt;p&gt;公式 54 ：扰动后的分布可表示为能量函数 $E(y)$ 的指数形式；能量越低的区域，概率密度越高（玻尔兹曼分布原理）&lt;/p&gt;
&lt;p&gt;公式 55 ：第一项表示原始高斯分布的能量项（二次型）；第二项表示扰动函数 $r(y)$ 对应的能量项&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果 $E_r(y)$ 相对于 $\frac{1}{2}(y−μ)^T\Sigma^{−1}(y−μ)$ 是平滑的，那么我们可以使用其在 $\mu$ 处的泰勒展开来近似它。一个充分条件是 $E_r(y)$ 的 Hessian 矩阵的特征值处处比 $\Sigma^{-1}$ 的特征值的模小得多。然后我们有
&lt;/p&gt;
$$
E_r(y) \approx E_r(\mu) + (y-\mu)g \tag{56}
$$&lt;p&gt;
其中 $g=\frac{\partial E_r(y′)}{\partial y′} ∣_{y′=μ}$。将此代入完整能量函数，
&lt;/p&gt;
$$
\begin{align*}
E(y)  &amp;\approx  \frac{1}{2}(y - \mu)^T \Sigma^{-1} (y - \mu) + (y - \mu)^T \mathbf{g} + \text{constant} \tag{57} \\
&amp;= \frac{1}{2} y^T \Sigma^{-1} y - \frac{1}{2} y^T \Sigma^{-1} \mu - \frac{1}{2} \mu^T \Sigma^{-1} y + \frac{1}{2} y^T \Sigma^{-1} \Sigma \mathbf{g} + \frac{1}{2} \bf{g}^T \Sigma \Sigma^{-1} y + \text{constant} \tag{58} \\
&amp;= \frac{1}{2} (y - \mu + \Sigma \mathbf{g})^T \Sigma^{-1} (y - \mu + \Sigma \mathbf{g}) + \text{constant}. \tag{59} 
\end{align*}
$$&lt;p&gt;
这对应于一个高斯分布，
&lt;/p&gt;
$$
\tilde{p}(y \mid x^{(t)}) \approx \mathcal{N}(y; \mu - \Sigma g, \Sigma). \tag{60}
$$&lt;p&gt;
将结果代回原始形式，得到：
&lt;/p&gt;
$$
\tilde{p}\left(x^{(t-1)} \mid x^{(t)}\right) \approx \mathcal{N}\left(x^{(t-1)}; \mathbf{f}_{\mu}\left(x^{(t)}, t\right) + \mathbf{f}_{\Sigma}\left(x^{(t)}, t\right) \left.\frac{\partial \log r\left(x^{(t-1)&#39;}\right)}{\partial x^{(t-1)&#39;}}\right|_{x^{(t-1)&#39;} = \mathbf{f}_{\mu}\left(x^{(t)}, t\right)}, \mathbf{f}_{\Sigma}\left(x^{(t)}, t\right)\right). \quad (61)
$$&lt;h4 id=&#34;实验细节&#34;&gt;实验细节
&lt;/h4&gt;&lt;p&gt;用的卷积神经网络，数据集不再进行介绍。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
